%-------------------------------------------
% Document class
\documentclass[publish]{smj}

%-------------------------------------------
% Personal packages and definitions
\usepackage{amsfonts}
\usepackage{float}
\usepackage[section]{placeins}
\usepackage{bm}
\DeclareRobustCommand{\rchi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}}

% Algorithm
\usepackage[english, vlined, ruled]{algorithm2e}
\SetAlCapSty{}

\usepackage{booktabs}
\usepackage{tabularx}
\newcolumntype{C}{>{\centering\arraybackslash}X}

% -------------------------------------------
% Authors and affiliations
\Author{Eduardo E. Ribeiro Jr\Affil{1},
  Walmes M. Zeviani\Affil{2},
  Wagner H. Bonat\Affil{2},
  Clarice G. B. Dem\'{e}trio\Affil{1}
  and John Hinde\Affil{3}
}
\AuthorRunning{Ribeiro Jr \textrm{et al.}}

\Affiliations{
\item Department of Exact Sciences,
  University of S\~{a}o Paulo - ESALQ,
  Piracicaba,
  SP, Brazil
\item Department of Statistics,
  Paran\'{a} Federal University,
  Curitiba,
  PR, Brazil
\item School of Mathematics, Statistics and Applied Mathematics,
  National University of Ireland,
  Galway,
  Galway, Ireland
}

%-------------------------------------------
% Corresponding author
\CorrAddress{Eduardo E. Ribeiro Jr,
             Department of Exact Sciences,
             Luiz de Queiroz College of Agriculture - ESALQ,
             University of S\~{a}o Paulo - USP,
             Piracicaba, S\~{a}o Paulo,
             P\'{a}dua Dias, 11, Avenue, CEP--13.418-900
             Brazil}
\CorrEmail{jreduardo@usp.br}
\CorrPhone{(+55)\;41\;9\;8711\;9034}
\CorrFax{(+55)\;19\;3447\;6021}

%-------------------------------------------
% Title and shor title
\Title{Reparametrization of COM-Poisson Regression Models with
  Applications in the Analysis of Experimental Data}
\TitleRunning{Reparametrization of COM-Poisson Regression Models}

% -------------------------------------------
% Abstract
\Abstract{ In the analysis of count data often the equidispersion
  assumption is not suitable, hence the Poisson regression model is
  inappropriate.  As a generalization of the Poisson distribution the
  COM-Poisson distribution can deal with under-, equi- and overdispersed
  count data.  It is a member of the exponential family of distributions
  and has the Poisson and geometric distributions as special cases, as
  well as the Bernoulli distribution as a limiting case.  In spite of
  the nice properties of the COM-Poisson distribution, its location
  parameter does not correspond to the expectation, which complicates
  the interpretation of regression models specified using this
  distribution. In this paper, we propose a straightforward
  reparametrization of the COM-Poisson distribution based on an
  approximation to the expectation of this distribution. The main
  advantage of our new parametrization is the straightforward
  interpretation of the regression coefficients in terms of the
  expectation of the count response variable, as usual in the context of
  generalized linear models. Furthermore, the estimation and inference
  for the new COM-Poisson regression model can be done based on the
  likelihood paradigm. We carried out simulation studies to verify the
  finite sample properties of the maximum likelihood estimators. The
  results from our simulation study show that the maximum likeli-hood
  estimators are unbiased and consistent for both regression and
  dispersion parameters. We observed that the empirical correlation
  between the regression and dispersion parameter estimators is close to
  zero, which suggests that these parameters are orthogonal. We
  illustrate the application of the proposed model through the analysis
  of three data sets with over-, under- and equidispersed count data.
  The study of distribution properties through a consideration of
  dispersion, zero-inflated and heavy tail indices, together with the
  results of data analysis show the flexibility over standard
  approaches. Therefore, we encourage the application of the new
  parametrization for the analysis of count data in the context of
  COM-Poisson regression models.  The computational routines for fitting
  the original and new version of the COM-Poisson regression model and
  the analyzed data sets are available in the supplementary material.
}

%-------------------------------------------
% Key-words
\Keywords{ COM-Poisson, Count data, Likelihood inference,
  Overdispersion, Underdispersion. }

<<setup, include=FALSE>>=

library(knitr)
library(xtable)
options(digits = 3, OutDec = ".",
        xtable.caption.placement = "top",
        xtable.booktabs = TRUE,
        xtable.sanitize.text.function = identity,
        xtable.size = "small",
        xtable.math.style.negative = TRUE)
opts_chunk$set(
    warning = FALSE,
    message = FALSE,
    echo = FALSE,
    results = "hide",
    fig.width = 7,
    fig.height = 5,
    out.width = "1\\textwidth",
    fig.align = "center",
    fig.pos = "!htp",
    # dev = "tikz"
    dev.args = list(family = "Palatino")
    )

divide_matrix <- function(mat, divide) {
    mat1 <- mat[, 1:divide]
    mat2 <- mat[, (divide + 1):ncol(mat)]
    nc1 <- ncol(mat1)
    nc2 <- ncol(mat2)
    cdiff <- abs(ncol(mat1) - ncol(mat2))
    if (cdiff) {
        complete <- matrix(NA, nrow = nrow(mat), ncol = cdiff)
        colnames(complete) <- rep(" ", cdiff)
        if (nc1 < nc2) {
            mat1 <- cbind(mat1, complete)
        } else {
            mat2 <- cbind(mat2, complete)
        }
    }
    return(list(mat1, mat2))
}

source("./codes/lattice-panels.R")
source("./codes/functions.R")

# Colors for legends
cols <- trellis.par.get("superpose.line")$col

# Useful packages
library(bbmle)
library(multcomp)
library(plyr)
library(tidyr)
library(dplyr)

@

%-----------------------------------------------------------------------

\begin{document}

\maketitle

\section{Introduction}
\label{introduction}

Count data are random variables that assume non-negative integer values
and represent the number of times an event occurs in the observation
period. This kind of data is common in crop sciences, such as the number
of grains produced by a plant, number of fruits produced by a tree,
number of insects captured by a trap, to cite but a few.  Since the
seminal paper of \citet{Nelder1972} where the class of the generalized
linear models (GLMs) was introduced, the analysis of count data often
employs the Poisson regression model. This model provides a suitable
strategy for the analysis of count data and an efficient Newton scoring
algorithm can be used for fitting the model.

In spite of the advantages of the Poisson regression model, the Poisson
distribution has only one parameter, which represents both the
expectation and variance of the count random variable.  This restriction
on the relationship between the expectation and variance induced by the
Poisson distribution is referred as equidispersion.  However, in
practical data analysis such a restriction can be unsuitable, since the
observed data can present  variance both smaller or larger than the
expectation, leading to the cases of under and overdispersion,
respectively.  The main problem of the application of the Poisson
regression model to non-equidispersed count data is that the standard
errors associated with the regression coefficients are inconsistently
estimated, which in turn can lead to misleading
inferences~\citep{Winkelmann1995, Bonat2017}.

In practice, overdispersion is largely reported in the literature and
may occur due to the absence of relevant covariates, heterogeneity of
sampling units, different observational periods/regions not considered
in the analysis, and excess of zeros~\citep{Hinde1998}.  The case of
underdispersion is less report in the literature, however, it has been
of increasing interest in the statistical community. The processes that
reduce the variability are not as well-known as those leading to extra
variability.  For this reason, there are few approaches to deal with
underdispersed count data. The explanatory mechanisms leading to
underdispersion may be related to the underlying stochastic process
generating the count data.  When the time between events is not
exponentially distributed, the number of events can be over or
underdispersed, this process motivated the class of duration dependence
models~\citep{Winkelmann1995}.  Another possible explanation of
underdispersion is when the responses correspond to order statistics of
component observations, such as maxima of Poisson distributed
counts~\citep{Steutel1989}.

The strategies for constructing alternative count distributions are
related with the causes of the non-equidispersion. Specfically for the
overdispersion case Poisson mixture models are widely applied.  One
popular example of this approach is the negative-binomial model, where
the expectation of the Poisson distribution is assumed to be gamma
distributed.  However, other distributions can be used to represent the
random variation.  For example the Poisson-Tweedie
model~\citep{Bonat2017} and its special cases as the Poisson
inverse-Gaussian and Neyman-Type A assume that the random effects are
Tweedie, inverse Gaussian and Poisson distributed, respectively.  The
Gamma-Count distribution assumes a gamma distribution for the time
between events, thus it can deal with underdispersed as well as
overdispersed count data~\citep{Zeviani2014}. The COM-Poisson
distribution is obtained by a generalization of the Poisson distribution
allowing for a non-linear decrease in the ratios of successive
probabilities~\citep{Shmueli2005}.

The COM-Poisson distribution is a member of the exponential family and
it has the Poisson and geometric distributions as special cases, as well
as the Bernoulli distribution as a limiting case. It can deal with both
under and overdispersed count data.  Some recently applications of the
COM-Poisson distribution include \citet{Lord2010} for the analysis of
traffic crash data, \citet{Sellers2010} for the modelling of airfreight
breakage and book purchases, and \citet{Huang2017} to the analysis of
attendance data, takeover birds and cotton boll counts.  The main
disadvantage of the COM-Poisson regression model as presented
in~\citet{Sellers2010} is that its location parameter does not
correspond to the expectation of the distribution, which complicates the
interpretation of regression models and means that they are not
comparable with standard approaches such as the Poisson and negative
binomial regression models. \citet{Huang2017} proposed a
mean-parametrization of the COM-Poisson distribution in order to avoid
such an issue.  In this approach the mean parameter is obtained by
solving an non-linear equation defined as an infinite sum.
Consequently, it is computationally demanding and liable to numerical
problems.

The main goal of this article is to propose a novel COM-Poisson
parametrization based on the mean approximation presented by
\citet{Shmueli2005}. In this parametrization, the probability mass
function is written in terms of $\mu$ and $\phi$, where $\mu$ is the
expectation and $\phi$ is a dispersion parameter. In contrast to the
original parametrization, the proposed parametrization leads to
regression coefficients directly associated with the expectation of the
response variable, as usual in the context of generalized linear models.
Consequently, the obtained regression coefficients are comparable with
the ones obtained by standard approaches, such as the Poisson and
negative binomial regression models.  Furthermore, our novel COM-Poisson
parametrization is simpler than the strategy proposed
by~\cite{Huang2017}, since it does not require any numerical method for
solving non-linear equations, and we show the attractive properties like
the orthogonality between dispersion and regression parameters and
consistency and asymptotic normality of the maximum likelihood
estimators are retained.

This paper is organized as follows. In \autoref{background} we present
the COM-Poisson distribution and the strategy proposed
by~\cite{Huang2017}.  The proposed reparametrization, assessment of
moment approximations, and study of distribution properties are
considered in the~\autoref{reparametrization}. In
the~\autoref{estimation-and-inference} we present estimation and
inference for the novel COM-Poisson regression model based on the
likelihood paradigm. The properties of the maximum likelihood estimators
and the orthogonality property are assessed
in~\autoref{simulation-study} through simulation studies.  We illustrate
the application of the new COM-Poisson regression model through the
analysis of three data sets. We provide an \texttt{R} implementation of
the COM-Poisson and reparameterized COM-Poisson regression models as
well as the analyzed data sets in the supplementary
material.\footnote{Available on
  \texttt{\url{http://www.leg.ufpr.br/~eduardojr/papercompanions}}
  \label{papercompanion}.}.

\section{Background}
\label{background}

The COM-Poisson distribution generalizes the Poisson distribution in
terms of the ratio between the probabilities of two consecutive events
by adding an extra dispersion parameter~\citep{Sellers2010}.  Let $Y$ be
a COM-Poisson random variable, then
$$\frac{\Pr(Y=y-1)}{\Pr(Y=y)} = \frac{y^\nu}{\lambda}$$ while for the
Poisson distribution this ratio is $\frac{y}{\lambda}$ corresponding to
$\nu=1$.  It allows the COM-Poisson distribution deals with
non-equidispersed count data.  The probability mass function of the
COM-Poisson distribution is given by
\begin{equation}
  \label{eqn:pmf-cmp}
  \Pr(Y=y \mid \lambda, \nu) = \frac{\lambda^y}{(y!)^\nu Z(\lambda, \nu)},
  \qquad y = 0, 1, 2, \ldots,
\end{equation}
where $\lambda > 0$, $\nu \geq 0$ and $Z(\lambda, \nu) =
\sum_{j=0}^\infty \frac{\lambda^j}{(j!)^\nu}$ is a normalizing
constant that depends on both parameters.

The $Z(\lambda, \nu)$ series diverges theoretically only when $\nu=0$
and $\lambda \geq 1$, but numerically for small values of $\nu$ combined
with large values of $\lambda$, the sum is so huge it causes
overflow. \autoref{tab:convergenceZ} shows the values of the normalizing
constants using one thousand increments, that is,
$\sum_{j=0}^{1000}\lambda^j/(j!)^\nu$ for different values of $\lambda$
and $\phi$.

<<convergenceZ, echo=FALSE>>=

#-----------------------------------------------------------------------
# Convergence of Z(lambda, nu) constant
computeZ <- function(lambda, nu, maxit = 1e4, tol = 1e-5) {
    z <- vector("numeric", maxit)
    j = 1
    z[j] <- exp(j * log(lambda) - nu * lfactorial(j))
    while (abs(z[j] - 0) > tol && j <= maxit) {
        j = j + 1
        z[j] <- exp(j * log(lambda) - nu * lfactorial(j))
    }
    if (lambda == 1 & nu == 0) z <- Inf
    return(sum(z, na.rm = TRUE) + 1)
}

grid <- expand.grid(
    lambda = c(0.5, 1, 5, 10, 30, 50),
    nu = seq(0, 1, length.out = 11))
grid$z <- apply(grid, 1, function(par) {
    computeZ(lambda = par[1], nu = par[2], maxit = 1e4)
})

xt <- xtabs(z ~ nu + lambda, data = grid)

@

\begin{table}[ht]
\centering
\caption{Values for $Z(\lambda, \nu)$ constant (numerically computed) for values of $\lambda$ (0.5 to 50) and $\phi$ (0 to 1)}
\label{tab:convergenceZ}
\begingroup\small
\begin{tabularx}{\textwidth}{C|CCCCCC}
  \toprule

<<results="asis">>=
caption <- paste(
    "Values for $Z(\\lambda, \\nu)$ constant (numerically computed)",
    "for values of $\\lambda$ (0.5 to 50) and $\\phi$ (0 to 1)")

# Remove excessive scientific notation
xt2 <- gsub("e\\+00", "    ", format(xt))
xt2 <- gsub("Inf", "divergent$^{**}$", format(xt2))
xt2[1, -1] <- "divergent$^{*\\,\\,\\,}$"

print(xtable(xt2, digits = -2,
             caption = caption,
             align = "C|CCCCCC",
             # align = "c|cccccc",
             label = "tab:convergenceZ"),
      include.colnames = FALSE,
      tabular.environment = "tabularx",
      only.contents = TRUE,
      width = "\\textwidth",
      add.to.row = list(
          pos = list(0, 0),
          command = c(
              "& \\multicolumn{6}{c}{$\\bm{\\lambda}$} \\\\\n",
              "$\\bm{\\nu}$ & 0.5 & 1 & 5 & 10 & 30 & 50 \\\\\n")
      ))

@

\end{tabularx}
\endgroup

\footnotesize \raggedright
divergent$^{*}$ is a mathematically divergent series; and
divergent$^{**}$ a numerically divergent series.
\end{table}

In the first line of \autoref{tab:convergenceZ} we have mathematically
divergent series, because $\sum_{j=0}^\infty\lambda^j$ is divergent when
$\lambda \geq 1$.  In other cases the series diverges numerically, due
to the computational storage limitation.  For both forms of divergence
it is impossible to compute probabilities, therefore, this acts as a
restriction on the parameter space.

An undesirable feature of the COM-Poisson distribution is that the
moments cannot be obtained in closed form. \citet{Shmueli2005} and
\citet{Sellers2010} using an asymptotic approximation for
$Z(\lambda,\nu)$, showed that the expectation and variance of the
COM-Poisson distribution can be approximated by
\begin{equation}
  \label{eqn:mean-approx}
  \text{E}(Y) \approx \lambda^{1/\nu} - \frac{\nu - 1}{2\nu} \qquad
  \textrm{and} \qquad
  \textrm{Var}(Y) \approx \frac{\lambda^{1/\nu}}{\nu}\,,
\end{equation} which is particularly accurate for  $\nu \leq 1$ or
$\lambda > 10$. The authors also argue that the mean-variance
relationship can be approximate by
$\frac{1}{\nu}\text{E}(Y)$. In~\autoref{reparametrization}, we assess
the accuracy of these approximations.

The COM-Poisson regression model was proposed by~\citet{Sellers2010},
using the original parametrization. In this case, the COM-Poisson
regression model is $\log(\lambda_i) = \bm{x}_i^\top \bm{\beta}$ and the
relationship between E$(Y_i)$ and $\bm{x}_i$ is modelled indirectly.
\citet{Huang2017} shows how to model directly the expectation of the
COM-Poisson distribution in a suitable parametrization. In the
\autoref{eqn:pmf-cmp}, \Citeauthor{Huang2017} proposes that the
parameter $\lambda$ as a function of $\mu$ and $\nu$, is given by the
solution to
\begin{equation*}
  \sum_{j=0}^{\infty} (j - \mu) \frac{\lambda^j}{(y!)^\nu} = 0\,.
\end{equation*}
Thus the mean-parametrized COM-Poisson regression model is $\log(\mu_i)
= \bm{x}_i^\top \bm{\beta}$. In this article, we propose an alternative
mean-parametrization of the COM-Poisson distribution in order to avoid the
limitations of the original parametrization and the numerical complexity
of the \Citeauthor{Huang2017}'s approach.

\section{Reparametrized COM-Poisson regression model}
\label{reparametrization}

The proposed reparametrization of COM-Poisson models is based on
the mean approximation (\autoref{eqn:mean-approx}). We introduced a new
parameter $\mu$, using this approximation,
\begin{equation}
  \label{eqn:repar-cmp}
  \mu = h(\lambda, \nu) = \lambda^{1/\nu} - \frac{\nu - 1}{2\nu}
  \quad \Rightarrow \quad
  \lambda = h^{-1}(\mu, \nu) = \left (\mu +
    \frac{(\nu - 1)}{2\nu} \right )^\nu.
\end{equation}
The dispersion parameter is taken on the log scale for computational
convenience, thus $\phi = \log(\nu)$, $\phi \in \mathbb{R}$. The
interpretation of $\phi$ is the same as the $\nu$, but on another
scale. For $\phi < 0$ and $\phi > 0$ we have the overdispersed and
underdispersed cases, respectively.  When $\phi=0$ we have Poisson
distribution as a special case.

<<data-approx, include=FALSE, cache=TRUE>>=

#-----------------------------------------------------------------------
# Study the approximation

#-------------------------------------------
# Mean and variance relationship
aux <- expand.grid(
    mu = seq(3, 30, length.out = 55),
    phi = seq(-1.5, 1.8, length.out = 50))

moments <- mapply(FUN = calc_moments,
                  mu = aux$mu,
                  phi = aux$phi,
                  MoreArgs = list(sumto = 300),
                  SIMPLIFY = FALSE)
grid <- cbind(aux, t(do.call(cbind, moments)))
grid <- transform(grid, va = mu / exp(phi))

@

In order to assess the accuracy of the moment approximations
(\autoref{eqn:mean-approx}), \autoref{fig:approx-plot} presents the
quadratic errors for (a) expectation  and (b) variance.  The
quadratic errors were obtained by $[\mu - \text{E}(Y)]^2$ for the
expectation and by $[ \mu \exp(-\phi) - \text{Var}(Y)]^2$ for the
variance. In both cases $\text{E}(Y)$ and Var$(Y)$ were computed
numerically. The dotted lines represent the border between the regions
$\nu \leq 1$ and $\lambda > 10^\nu$, in the $\mu$ and $\phi$ scale.

<<approx-plot, fig.height=3, fig.width=6, out.width=".8\\textwidth", fig.cap="Quadratic errors for the approximation of the (a) expectation and (b) variance. Dotted lines represent the restriction for suitable approximations given by \\cite{Shmueli2005}.">>=

#-------------------------------------------
# Errors in approximations for E[Y] and V[Y]
grid <- transform(grid,
                  emu = (mu - mean)^2,
                  eva = (va - var)^2)

myreg <- colorRampPalette(c("gray90",  "gray20"))
xy1 <- levelplot(emu ~ phi + mu, data = grid,
                 aspect = "fill",
                 col.regions = myreg,
                 xlab = expression(phi),
                 ylab = expression(mu),
                 sub = "(a)",
                 colorkey = list(space = "top"),
                 par.settings = ps2,
                 panel = function(x, y, z, ...) {
                     panel.levelplot(x, y, z, ...)
                     panel.curve(10 - ( exp(x) - 1)/(2 * exp(x)),
                                 lty = 2)
                     panel.abline(v = 0, lty = 2)
                 })

xy2 <- levelplot(eva ~ phi + mu, data = grid,
                 aspect = "fill",
                 col.regions = myreg,
                 xlab = expression(phi),
                 ylab = expression(mu),
                 sub = "(b)",
                 colorkey = list(space = "top"),
                 par.settings = ps2,
                 panel = function(x, y, z, ...) {
                     panel.levelplot(x, y, z, ...)
                     panel.curve((10 - ( exp(x) - 1)/
                                  (2 * exp(x)))/exp(x), lty = 2)
                     panel.abline(v = 0, lty = 2)
                 })

print(xy1, split = c(1, 1, 2, 1), more = TRUE)
print(xy2, split = c(2, 1, 2, 1), more = FALSE)

@

The results in \autoref{fig:approx-plot} show that the mean
approximation is accurate, the largest quadratic error is
\Sexpr{max(grid[["emu"]])} for the parameter values evaluated. For the
variance approximation, the largest quadratic error was
\Sexpr{max(grid[["eva"]])} and it occurs for negative values of
$\phi$. Interestingly, the errors are larger for negative values of
$\phi$ and present no clear relation with $\mu$, as opposed to the
regions gives by \citet{Shmueli2005} ($\phi \leq 0$ and
$\mu > 10 - \frac{\exp(\phi) - 1}{2\exp(\phi)}$).

The results presented in \autoref{fig:approx-plot}(a) support the
proposed reparametrization. Replacing $\lambda$ and $\nu$ as function of
$\mu$ and $\phi$ in \autoref{eqn:pmf-cmp}, the reparametrized
distribution takes the form
\begin{equation}
  \label{eqn:pmf-cmpmu}
  \Pr(Y=y \mid \mu, \phi) =
  \left ( \mu +\frac{ e^\phi-1}{2e^\phi} \right )^{ye^\phi}
  \frac{(y!)^{-e^\phi}}{Z(\mu, \phi)},
  \qquad y = 0, 1, 2, \ldots\,,
\end{equation} where $\mu > 0$. We denote this distribution as
COM-Poisson$_\mu$. In \autoref{fig:pmf-cmp}, we show the shapes of
COM-Poisson$_\mu$ distribution.

<<pmf-cmp, fig.height=3, fig.width=7, fig.cap="Shapes of the COM-Poisson distribution for different parameter values.">>=

#-------------------------------------------
# COM-Poisson probabilities
parg <- expand.grid(mu = c(2, 8, 15), phi = c(-0.7, 0, 0.7))
y <- 0:30
py <- mapply(FUN = dcmp,
             mu = parg$mu,
             phi = parg$phi,
             MoreArgs = list(y = y, sumto = 100),
             SIMPLIFY = FALSE)
parg <- cbind(parg[rep(1:nrow(parg), each = length(y)), ],
              y = y, py = unlist(py))

leg_phi <- parse(
    text = paste("phi == \"",
                 formatC(unique(parg$phi), 1, format = "f"),
                 "\""))
barchart(py ~ y | factor(mu),
         groups = factor(phi),
         data = parg,
         horizontal = FALSE,
         layout = c(NA, 1),
         as.table = TRUE,
         axis = axis.grid,
         origin = 0,
         xlim = extendrange(y, f = 0.01),
         border = "transparent",
         # scales = list(x = list(at = pretty(y))),
         ylab = expression(P(Y==y)),
         xlab = expression(y),
         par.settings = list(
             superpose.polygon = list(
                 col = c("gray40", "gray60", "gray10")),
             superpose.line = list(
                 col = c("gray40", "gray60", "gray10"),
                 lwd = 2)
         ),
         auto.key = list(
             columns = 3,
             rectangles = FALSE,
             lines = TRUE,
             text = leg_phi
         ),
         strip = strip.custom(
             strip.names = TRUE,
             var.name = expression(mu == ""),
             sep = ""))

@

In order to explore the flexibility of the COM-Poisson model to deal
with real count data, we compute indexes for dispersion (DI),
zero-inflation (ZI) and heavy-tail (HI), which are respectively given by
\begin{equation*}
\text{DI} = \frac{\text{Var}(Y)}{\text{E}(Y)}, \quad
\text{ZI} = 1 + \frac{\log \Pr(Y = 0)}{\text{E}(Y)}
  \quad \text{and} \quad
\text{HT} = \frac{\Pr(Y=y+1)}{\Pr(Y=y)}\quad \text{for} \quad y \to
\infty.
\end{equation*}
These indexes are defined in relation to the Poisson distribution. Thus,
the dispersion index indicates overdispersion for $\text{DI} > 1$,
underdispersion for $\text{DI} < 1$ and equidispersion for
$\text{DI} = 1$. The zero-inflation index indicates zero-inflation for
$\text{ZI} > 0$, zero-deflation for $\text{ZI} < 0$ and no excess of
zeros for $\text{ZI} = 0$. Finally, heavy-tail index indicates a
heavy-tail distribution for $\text{HT} \to 1$ when $y \to \infty$.

These indexes are discussed by \citet{Bonat2017} to study the
flexibility of Poisson-Tweedie distribution, and \citet{Puig2006} to
describe count distributions. Regarding the COM-Poisson$_\mu$
distribution, in \autoref{fig:indexes-plot} we present the relationship
between (a) mean and variance, (b--c) the dispersion and zero-inflation
indexes for different values of $\mu$ and $\phi$, and (d) heavy-tail
index for $\mu=25$ and different values of $y$ and $\phi$.

<<compute-indexes, include=FALSE, cache=TRUE>>=

# Dispersion index
grid <- transform(grid, dispersion_index = var / mean)

# Zero-inflation index
prob0 <- mapply(FUN = dcmp,
                mu = grid$mu,
                phi = grid$phi,
                MoreArgs = list(y = 0, sumto = 300),
                SIMPLIFY = FALSE)
prob0 <- unlist(prob0)
grid <- transform(grid, zero_index = 1 + log(prob0) / mean)

# Heavy tail-index
hi_fun <- Vectorize(FUN = function(x, mu, phi, sumto) {
    probs <- dcmp(y = c(x, x + 1), mu = mu, phi = phi, sumto = sumto)
    probs[2] / probs[1]
}, vectorize.args = "x")

x <- 40:150
his <- mapply(FUN = hi_fun,
              mu = grid$mu,
              phi = grid$phi,
              MoreArgs = list(x = x, sumto = 300),
              SIMPLIFY = FALSE)

aux1 <- lapply(his, function(k) data.frame("x" = x, "heavy_index" = k))
aux1 <- do.call(rbind, aux1)
aux2 <- grid[c("mu", "phi")][rep(1:nrow(grid), each = length(x)), ]
heavy_data <- cbind(aux1, aux2)

@

<<indexes-plot, fig.width=9.5, fig.height=4, fig.cap="Indexes for COM-Poisson distribution. (a) Mean and variance relationship, (b--d) dispersion, zero-inflation and heavy-tail indexes for different parameter values. Dotted lines represents the Poisson special case.">>=

#-----------------------------------------------------------------------
# Study indexes on the reparametrized COM-Poisson distribution
mygpar <- gpar(cex = 1.2)
psaux <- modifyList(
    ps0, list(superpose.line = list(
                  col = myreg(length(unique(grid$phi)))),
              layout.widths = list(
                  left.padding = -0.5)
              )
)

#-------------------------------------------
# Mean and variance relationship
xy1 <- xyplot(var ~ mu | "Mean-Variance",
              groups = phi,
              data = grid,
              type = "l",
              lwd = 2,
              axis = axis.grid,
              xlab = expression(mu),
              ylab = "",
              # ylab = expression(Var(Y)),
              scales = list(y = list(rot = 90)),
              sub = "(a)",
              legend = list(
                  top = list(
                      fun = draw.colorkey,
                      args = list(
                          key = list(
                              space = "top",
                              col = myreg(length(unique(grid$phi))),
                              at = unique(grid$phi),
                              draw = FALSE)))),
              par.settings = psaux,
              panel = function(x, y, ...) {
                  panel.xyplot(x, y, ...)
                  panel.curve(1*x, min(x), max(x), lty = 2)
              },
              page = function(...) {
                  grid.text(expression(phi),
                            just = "bottom",
                            gp = mygpar,
                            x = unit(0.020, "npc"),
                            y = unit(0.875, "npc"))
              })

#-------------------------------------------
# Dispersion index (DI)
xy2 <- xyplot(var / mean ~ mu | "Dispersion index",
              groups = phi,
              data = grid,
              type = "l",
              lwd = 2,
              axis = axis.grid,
              xlab = expression(mu),
              ylab = "",
              scales = list(y = list(rot = 90)),
              sub = "(b)",
              legend = list(
                  top = list(
                      fun = draw.colorkey,
                      args = list(
                          key = list(
                              space = "top",
                              col = myreg(length(unique(grid$phi))),
                              at = unique(grid$phi),
                              draw = FALSE)))),
              par.settings = psaux,
              panel = function(x, y, ...) {
                  panel.xyplot(x, y, ...)
                  panel.curve(1 + 0*x, min(x), max(x), lty = 2)
              },
              page = function(...) {
                  grid.text(expression(phi),
                            just = "bottom",
                            gp = mygpar,
                            x = unit(0.020, "npc"),
                            y = unit(0.875, "npc"))
              })

#-------------------------------------------
# Zero-inflation index
xy3 <- xyplot(zero_index ~ mu | "Zero-inflation index",
              groups = phi,
              data = grid,
              type = "l",
              lwd = 2,
              axis = axis.grid,
              xlab = expression(mu),
              ylab = "",
              scales = list(y = list(rot = 90)),
              sub = "(c)",
              legend = list(
                  top = list(
                      fun = draw.colorkey,
                      args = list(
                          key = list(
                              space = "top",
                              col = myreg(length(unique(grid$phi))),
                              at = unique(grid$phi),
                              draw = FALSE)))),
              par.settings = psaux,
              panel = function(x, y, ...) {
                  panel.xyplot(x, y, ...)
                  panel.curve(0*x, min(x), max(x), lty = 2)
              },
              page = function(...) {
                  grid.text(expression(phi),
                            just = "bottom",
                            gp = mygpar,
                            x = unit(0.020, "npc"),
                            y = unit(0.875, "npc"))
              })

#-------------------------------------------
# Heavy tail-index
# Choose one specific mu of unique(grid$mu)
choosemu <- 25
xy4 <- xyplot(heavy_index ~ x | "Heavy-tail index",
              groups = phi,
              # data = heavy_data,
              data = subset(heavy_data, mu == choosemu),
              type = "l",
              lwd = 2,
              axis = axis.grid,
              xlab = expression(y),
              ylab = "",
              scales = list(y = list(rot = 90)),
              sub = "(d)",
              legend = list(
                  top = list(
                      fun = draw.colorkey,
                      args = list(
                          key = list(
                              space = "top",
                              col = myreg(length(unique(grid$phi))),
                              at = unique(grid$phi),
                              draw = FALSE)))),
              par.settings = psaux,
              panel = function(x, y, ...) {
                  panel.xyplot(x, y, ...)
                  panel.curve(choosemu / (x + 1), from = min(x),
                              to = max(x), lty = 2)
              },
              page = function(...) {
                  grid.text(expression(phi),
                            just = "bottom",
                            gp = mygpar,
                            x = unit(0.020, "npc"),
                            y = unit(0.875, "npc"))
              })

print(xy1, split = c(1, 1, 4, 1), more = TRUE)
print(xy2, split = c(2, 1, 4, 1), more = TRUE)
print(xy3, split = c(3, 1, 4, 1), more = TRUE)
print(xy4, split = c(4, 1, 4, 1), more = FALSE)

midi <- min(grid$dispersion_index)
madi <- max(grid$dispersion_index)

@

\autoref{fig:indexes-plot} shows that the indexes are slightly dependent
on the expected values and tend to stabilize for large values of
$\mu$. Consequently, the mean and variance relationship
\autoref{fig:indexes-plot}(a) is proportional to the dispersion
parameter $\phi$. In terms of moments, this leads to a specification
indistinguishable from the quasi-Poisson regression model. The
dispersion indexes in \autoref{fig:indexes-plot}(b) show that the
distribution is suitable to deal to dispersed counts, of course. For the
parameter values evaluated the largest DI was \Sexpr{madi} and smallest
was \Sexpr{midi}. \autoref{fig:indexes-plot}(c) shows the COM-Poisson
can handle a limited amount of zero-inflation, in cases of
overdispersion ($\phi < 0$). On the other hand, for $\phi > 0$
(underdispersion) this distribution is suitable to deal with
zero-deflated counts. Heavy-tail indexes in
\autoref{fig:indexes-plot}(d) indicate the distribution is in general a
light-tailed distribution, i.e. $HT \to 0$ for $y \to \infty$.

\section{Estimation and Inference}
\label{estimation-and-inference}

In this section we describe the estimation and inference for the
two forms of the COM-Poisson regression model based on the maximum
likelihood method. The log-likelihood function for a set of independent
observations $y_i$, $i=1,2,\ldots,n$ from the COM-Poisson$_\mu$
distribution has the following form,
\begin{equation}
  \label{eqn:ll-rcmp}
  \ell = \ell(\bm{\beta}, \phi \mid \bm{y}) =
  e^\phi \left [
    \sum_{i=1}^n y_i
    \log \left( \mu_i + \frac{e^\phi-1}{2e^\phi} \right ) -
    \sum_{i=1}^n \log(y_i!) \right ] -
  \sum_{i=1}^n \log(Z(\mu_i, \phi)),
\end{equation}
where $\mu_i = \exp(\bm{x}_i^\top\bm{\beta})$, with
$\bm{x}_i^\top = (x_{i1},\, x_{i2},\, \ldots,\, x_{ip})$ is a vector of
known covariates for the $i$-th observation, and $(\bm{\beta},\, \phi)
\in \mathbb{R}^{p+1}$. The normalizing constant $Z(\mu_i, \phi)$ is
given by
\begin{equation}
\label{eqn:infseries}
  Z(\mu_i, \phi) = \sum_{j=0}^\infty \left [ \left (
    \mu_i + \frac{e^\phi - 1}{2e^\phi} \right )^{je^\phi}
  \frac{1}{(j!)^{e^\phi}} \right ].
\end{equation}
The evaluation of the log-likelihood function for each observation
involves the computation of the infinite series
(\autoref{eqn:infseries}).  Thus, the fitting procedure is computationally
expensive for regions of the parameter space where the convergence of
the infinite sum is slow.

Parameter estimation requires the numerical maximization of
\autoref{eqn:ll-rcmp}. Since the derivatives of $\ell$ cannot be
obtained in closed forms, we adopted the \texttt{BFGS}
algorithm~\citep{Nocedal1995} as implemented in the function
\texttt{optim()} for the statistical software \texttt{R}
\citep{Rcore2017}. Standard errors for the regression coefficients are
obtained based on the observed information matrix
$\mathcal{I}(\bm{\theta})$, where
$\mathcal{I}(\bm{\theta}) = -\mathcal{H}(\bm{\theta})$ (hessian matrix)
is computed numerically. Confidence intervals for $\hat{\mu}_i$ are
obtained by using the delta method~\citep[p. 89]{Pawitan2001}.

The parameter estimation for the COM-Poisson regression model in the
original parametrization is analogous to the one presented for the
COM-Poisson$_\mu$ distribution, however, it considers
\autoref{eqn:ll-rcmp} in terms of $\lambda$. Even for the standard
COM-Poisson distribution, the dispersion parameter is taken on the log
scale to avoid numerical issues.

In the applications we fitted the quasi-Poisson model
\citep{Wedderburn1974} as a baseline model. This approach is based on a
second-moment assumption that allows more flexibility to the model. In
this case the variance of the response variable is fixed by an
additional parameter $\sigma$, $\textrm{Var}(Y_i)=\sigma \mu_i$. These
models are fitted in the \texttt{R} software using the function
\texttt{glm(..., family = quasipoisson)}.

\section{Simulation study}
\label{simulation-study}

In this section we performed a simulation study to assess the properties
of the maximum likelihood estimators and orthogonality of the
reparametrized model as well as the flexibility of the COM-Poisson
regression model to deal with non-equidispersed count data.

We considered average counts varying from $3$ to $27$ according to a
regression model with a continuous and a categorical covariate. The
continuous covariate~($\bm{x}_1$) was generated as a sequence from $0$
to $1$ and of length equal to the sample size.  Similarly, the categorical
covariate~($\bm{x}_2$) was generated as a sequence of three values each
one repeated $n/3$ times (rounding up when required), where $n$ denotes
the sample size. Thus, the expectation of the COM-Poisson random
variable is given by
$\bm{\mu} = \exp(\beta_0 + \beta_1 \bm{x}_1 + \beta_{21} \bm{x}_{21} +
\beta_{22} \bm{x}_{22})$,
where $\bm{x}_{21}$ and $\bm{x}_{22}$ are dummy representing the levels
of $\bm{x}_2$.  The regression coefficients were fixed at the values,
$\beta_0 = 2$, $\beta_1 = 0.5$, $\beta_{21} = 0.8$ and
$\beta_{22} = -0.8$.

We designed four simulation scenarios by considering different values of
the dispersion parameter $\phi = -1.6, -1.0, 0.0$ and $1.8$.  Thus, we
have strong and moderate overdispersion, equidispersion, and
underdispersion, respectively.  \autoref{fig:justpars} shows the
variation of the average counts (left) and dispersion index (right) for
each value of the dispersion parameter considered in the simulation
study.  These configurations allow us to assess the properties of the
maximum likelihood estimators in extreme situations, such as high counts
and low dispersion, and low counts and high dispersion, but also in the
standard case of equidispersion.

%-----------------------------------------------------------------------
% Explain the simulation
% \begin{algorithm}
% \caption{Steps in simulation study.}
% \label{alg:simulation}
%   \Begin{
%   $\bm{\beta} = \begin{bmatrix} \beta_0 & \beta_1 & \beta_{21} &
%     \beta_{22} \end{bmatrix}^\top = \begin{bmatrix} 2.0 &
%     0.5 & 0.8 & -0.8 \end{bmatrix}^\top$\;
%   \For{$n \in \{50, 100, 300, 1000\}$}{
%     set $\bm{x}_1$ as a sequence, with $n$ elements, between $0$ and
%     $1$\;
%     set $\bm{x}_2$ as a repetition, with $n$ elements, of three
%     categories\;
%     compute $\bm{\mu}$ using $\bm{\mu} = \exp(\beta_0 + \beta_1 \bm{x}_1
%     +  \beta_{21} \bm{x}_{21} + \beta_{22} \bm{x}_{22})$, where
%     $\bm{x}_{21}$ and $\bm{x}_{22}$ are \textit{dummy} variable for
%     $\bm{x}_2$\;
%     \For{$\phi \in \{-1.6, -1.0, 0.0, 1.8\}$}{
%       \Repeat{$1000$ times}{
%         simulate $\bm{y}$ from COM-Poisson distribution with $\bm{\mu}$
%         and $\phi$ parameters\;
%         fit COM-Poisson$_\mu$ regression model to $\bm{y}$ data with
%         $\bm{X} = \begin{bmatrix} \bm{1} & \bm{x}_1 & \bm{x}_{21} &
%           \bm{x}_{22} \end{bmatrix}$ design matrix\;
%         get $\hat{\bm{\theta}} = \begin{bmatrix} \hat{\phi} & \hat{\beta}_0 &
%           \hat{\beta}_1 & \hat{\beta}_{21} & \hat{\beta}_{22}
%         \end{bmatrix}^\top$\;
%         get confidence intervals for $\hat{\bm{\theta}}$ by quadratic
%         approximation at the maximum likelihood estimate
%         (assumes $\hat{\bm{\theta}} \sim \mathcal{N}(\hat{\bm{\theta}},
%         \sqrt{-\bm{v}})$, where $\bm{v}$ is a diagonal of the inverse of
%         the Hessian matrix)\;
%       }
%     }
%   }
% }
% \end{algorithm}

<<load-simulation, cache=TRUE>>=

# Configuration
B <- 1000
beta <- c("b0" = 2, "b1" = 0.5, "b21" = 0.8, "b22" = -0.8)
phis <- c(0, -1.6, -1, 1.8)
names(phis) <- sprintf("phi=%s", phis)

sizes <- c(50, 100, 300, 1000)
names(sizes) <- sprintf("n=%s", sizes)

# Load results
results <- readRDS("./codes/simulation.rds")

@

<<justpars, fig.height=4, fig.width=9, fig.cap="Average counts (left) and dispersion indexes (right) for each scenario considered in the simulation study.">>=

#-------------------------------------------
# Justify the choices of the parameters simulation
x1 <- seq(0, 1, length.out = 100)
x2 <- rep(letters[1:3], length.out = 100)
X <- model.matrix(~ x1 + x2)
mu <- exp(X %*% beta)

daexplain <- ldply(lapply(phis, function(phi) {
    va <- compute_variance(mu, phi, sumto = 300)
    data.frame(mu = mu, va = va, di = va / mu, x1 = x1, x2 = x2)
}), .id = "phi")

labx2 <- expression("Level of categorical variable"~(x[2]))
labx1 <- expression("Values of continous variable"~(x[1]))

fl <- parse(text = gsub("=", "==", levels(daexplain$phi)))
xy1 <- xyplot(mu ~ x1, groups = x2,
              type = c("g", "l"),
              lwd = 2,
              xlab = labx1,
              ylab = "Average count",
              auto.key = list(
                  column = 3,
                  points = FALSE,
                  lines = TRUE,
                  title = labx2,
                  cex.title = 1.1
              ))
xy2 <- xyplot(di ~ x1 | phi, groups = x2,
              type = c("g", "l"),
              lwd = 2,
              # scales = "free",
              layout = c(2, 2),
              xlab = labx1,
              ylab = "Dispersion index",
              data = daexplain,
              strip = strip.custom(factor.levels = fl))

print(xy1, split = c(1, 1, 2, 1), more = TRUE)
print(xy2, split = c(2, 1, 2, 1), more = FALSE)

@

In order to check the consistency of the estimators we considered four
different sample sizes: $50$, $100$, $300$ and $1000$; generating $1000$
data sets in each case.  In \autoref{fig:bias-plot}, we show the bias of
the estimators for each simulation scenario (combination between values
of the dispersion parameter and samples sizes) along with the confidence
intervals calculated as average bias plus and minus $\Phi(0.975)$ times
the average standard error. The scales are standardized for each
parameter by dividing the average bias by the average standard error
obtained for the sample of size $50$.

<<bias-data, cache=TRUE>>=

#-----------------------------------------------------------------------
# Compute standardized bias
std <- lapply(results["n=50"], function(x) {
    ind <- names(x); names(ind) <- ind
    lapply(ind, function(y) {
        bhat <- t(vapply(x[[y]], "[[", double(5), "coef"))
        real <- matrix(c(phis[y], beta), byrow = TRUE,
                       nrow = B, ncol = 5)
        matrix(apply(bhat - real, 2, sd, na.rm = TRUE),
               byrow = TRUE, nrow = B, ncol = 5)
    })
})

aux <- ldply(lapply(results, function(x) {
    ind <- names(x); names(ind) <- ind
    out <- lapply(ind, function(y) {
        bhat <- t(vapply(x[[y]], "[[", double(5), "coef"))
        real <- matrix(c(phis[y], beta), byrow = TRUE,
                       nrow = B, ncol = 5)
        # (bhat - real)                 # raw bias
        (bhat - real) / std[[1]][[y]] # standardized bias
    })
    ldply(out, .id = "phi")
}), .id = "n")

# Organize the results
aux <- na.omit(aux)
bias <- gather(aux, param, bias, phi2:b22, factor_key = TRUE)
bias$phi <- ordered(bias$phi, c("phi=-1.6", "phi=-1", "phi=0",
                                "phi=1.8"))

@


The results in \autoref{fig:bias-plot} show that for all dispersion
levels, both the average bias and standard errors tend to $0$ as the
sample size increases. The estimators for the regression parameters are
unbiased, consistent and their empirical distributions are
symmetric. For the dispersion parameter, the estimator is asymptotically
unbiased; in small samples the parameter is overestimated and the
empirical distribution is slightly right-skewed.

<<bias-plot, cache=TRUE, fig.height=6.5, fig.width=8.5, fig.cap="Distributions of standardized bias (gray box-plots) and average with confidence intervals (black segments) by different sample sizes and dispersion levels.">>=

# Distributions of bias and average with confidence intervals
ci <- function(x) {
    ci <- mean(x) + c(-1, 0, 1) * qnorm(0.975) * sd(x)
    names(ci) <- c("lwr", "fit", "upr")
    ci
}
cidata <- aggregate(bias ~ n + phi + param, data = bias, ci)

key <- list(
    type = "o",
    divide = 1,
    columns = 4,
    title = "Sample size",
    cex.title = 1.1,
    lines = list(pch = c(21:24), cex = 0.8),
            text = list(names(sizes))
)

fl <- parse(text = gsub("=", "==", levels(bias$phi)))
yl <- parse(text = c("hat(phi)",
                     paste0("hat(beta)[", c(0, 1, 21, 22), "]")))


# Alternative graph (figure with all point is very large in size memory)
bwplot(param ~ bias | phi, groups = n, data = bias,
       pch = "|",
       grid = TRUE,
       as.table = TRUE,
       layout = c(4, 1),
       xlab = "Standardized Bias",
       fill = "gray80",
       col = "red",
       strip = strip.custom(factor.levels = fl),
       panel = panel.superpose,
       scales = list(y = list(labels = yl)),
       box.width = 0.05,
       whisker.width = 0.4,
       key = key,
       par.settings = list(
           plot.symbol = list(pch = 20, cex = 0.5, col = "gray70"),
           box.rectangle = list(col = "gray70"),
           box.umbrella=list(col = "gray70", lwd = 2)
       ),
       panel.groups = function(x, y, group.number, ...) {
           my.panel.bwplot(x, (y - 0.05) + (group.number - 2.5) / 5, ...)
           panel.abline(v = 0, lty = 2)
       }) +
    segplot(param ~ bias[, "lwr"] + bias[, "upr"] | phi,
            centers = bias[, "fit"],
            data = cidata,
            draw = FALSE,
            horizontal = TRUE,
            layout = c(4, 1),
            groups = n, gap = 0.3,
            key = key,
            pch = 21:24,
            cex = 0.7,
            lwd = 2,
            panel = function(...) {
                panel.groups.segplot(...)
                panel.abline(v = 0, lty = 2)
                panel.abline(h = 1:5, col = "lightgray", lty = 2)
            })

# xyplot(param ~ bias | phi, groups = n, data = bias,
#        panel = panel.superpose,
#        layout = c(4, 1),
#        key = key,
#        strip = strip.custom(factor.levels = fl),
#        scales = list(y = list(labels = yl)),
#        ylab = "",
#        xlab = "Standardized Bias",
#        jitter.y = TRUE,
#        factor = 0.1,
#        gap = 0.3,
#        alpha = 0.6,
#        col = "gray80",
#        cex = 0.7,
#        panel.groups = function(x, y, group.number,
#                                subscripts, gap, ...){
#            noise <- centfac(factor(levels(bias$n)), space = gap)
#            noise <- sort(noise)
#            panel.xyplot(x, y + noise[group.number], ...)
#        }) +
#     as.layer(
#         segplot(param ~ bias[, "lwr"] + bias[, "upr"] | phi,
#                 centers = bias[, "fit"],
#                 data = cidata,
#                 draw = FALSE,
#                 horizontal = TRUE,
#                 layout = c(4, 1),
#                 groups = n, gap = 0.3,
#                 key = key,
#                 pch = 21:24,
#                 cex = 0.7,
#                 lwd = 2,
#                 panel = function(...) {
#                     panel.groups.segplot(...)
#                     panel.abline(v = 0, lty = 2)
#                     panel.abline(h = 1:5, col = "lightgray", lty = 2)
#                 })
#     )

@

<<coverage-data>>=

#-----------------------------------------------------------------------
# Compute coverage rate
aux <- ldply(lapply(results, function(x) {
    ind <- names(x); names(ind) <- ind
    out <- lapply(ind, function(y) {
        ind <- lapply(x[[y]], function(z) {
            real <- c(phis[[y]], beta)
            cint <- z[["cint"]]
            ind <- as.integer(cint[, 1] < real & cint[, 2] > real)
            names(ind) <- c("phi2", names(beta))
            ind
        })
        do.call("rbind", ind)
    })
    ldply(out, .id = "phi")
}), .id = "n")

# Organize the results
aux <- na.omit(aux)
coverage <- gather(aux, param, coverage, phi2:b22, factor_key = TRUE)
coverage$phi <- ordered(coverage$phi, c("phi=-1.6", "phi=-1", "phi=0",
                                        "phi=1.8"))
coverage$n <- as.numeric(gsub("n=([0-9]+)", "\\1", coverage$n))
covdata <- aggregate(coverage ~ n + phi + param, data = coverage,
                     function(x) sum(x == 1) / length(x))

@


\autoref{fig:coverage-plot} presents the empirical coverage rate of the
asymptotic confidence intervals. The results show that for the
regression parameters the empirical coverage rates are close to the
nominal level of 95\% for sample sizes greater than $100$ and all
simulation scenarios. For the dispersion parameter the empirical
coverage rates are slightly lower than the nominal level; however, they
become closer to the nominal level for large samples.  The worst
scenario is when we have small sample size and strong overdispersed
counts.

<<coverage-plot, fig.height=4, fig.width=9, fig.cap="Coverage rate based on confidence intervals obtained by quadratic approximation for different sample sizes and dispersion levels.">>=

fl <- parse(text = gsub("=", "==", levels(bias$phi)))
yl <- parse(text = c("hat(phi)",
                     paste0("hat(beta)[", c(0, 1, 21, 22), "]")))
xyplot(coverage ~ n | param,
       type = c("g", "p", "l"),
       groups = phi,
       data = covdata,
       pch = 19,
       lwd = 1.5,
       layout = c(NA, 1),
       xlab = "Sample size",
       ylab = "Coverage rate",
       par.settings = list(
           layout.heights = list(strip = 1.5)
       ),
       key = list(
           column = 4,
           type = "o",
           divide = 1,
           text = list(fl),
           lines = list(pch = 19, col = cols[1:4])
       ),
       strip = strip.custom(
           factor.levels = yl
       ),
       panel = function(x, y, ...) {
           panel.xyplot(x, y, ...)
           panel.abline(h = 0.95, lty = 2)
       })

@

To check the orthogonality property we compute the covariance matrix
between maximum likelihood estimators
$\hat{\bm{\theta}} = (\hat{\bm{\beta}}, \phi)$, obtained by observed
information matrix,
Cov$(\hat{\bm{\theta}}) = \mathcal{I}^{-1}(\bm{\theta})$.
\autoref{fig:ortho-plot} shows the covariance between regression and
dispersion parameter estimators for each simulation scenario, on
the correlation scale. The correlations are close to zero in all cases
suggesting the orthogonality property for the reparametrized
model. Interesting results in the first panel shows that
cov($\hat{\beta}_{22}, \hat{\phi}$) is not very close to zero (values
between $-0.4$ and $0.2$) for strong overdispersion ($\phi = -1.6$).

<<ortho-plot, fig.height=4.5, fig.width=9, fig.cap="Empirical correlations between regression and dispersion parameters by different sample sizes and dispersion levels.">>=

#-----------------------------------------------------------------------
# Obtain the covariance between dispersion and regression parameters

aux <- ldply(
    lapply(results, function(x) {
    ind <- names(x); names(ind) <- ind
    out <- lapply(ind, function(y) {
        vcovs <- lapply(x[[y]], "[[", "vcov")
        index <- vapply(lapply(vcovs, is.na), sum, integer(1))
        vcovs <- vcovs[!index]
        corrs <- lapply(vcovs, cov2cor)
        covs <- do.call(rbind, lapply(vcovs, "[", 1, 2:5))
        cors <- do.call(rbind, lapply(corrs, "[", 1, 2:5))
        ldply(list("cov" = covs, "cor" = cors), .id = "scale")
    })
    ldply(out, .id = "phi")
}), .id = "n")
str(aux)

# Organize the results
dacov <- gather(aux, param, value, b0:b22, factor_key = TRUE)
dacov$phi <- ordered(dacov$phi,
                     c("phi=-1.6", "phi=-1", "phi=0", "phi=1.8"))
str(dacov)

# Distributions of covariance/correlations
fl <- parse(text = gsub("=", "==", levels(dacov$phi)))
yl <- parse(text = paste0("hat(beta)[", c(0, 1, 21, 22), "]"))

# Boxplots
bwplot(param ~ value | phi,
       groups = n,
       data = subset(dacov, scale == "cor"),
       box.width = 0.12,
       pch = "|",
       grid = TRUE,
       as.table = TRUE,
       layout = c(4, 1),
       xlab = "Empirical correlations with dispersion parameter estimator",
       fill = colorRampPalette(c("gray80",  "gray20"))(4),
       panel = panel.superpose,
       scales = list(y = list(labels = yl), x = "free"),
       whisker.width = 0.4,
       strip = strip.custom(factor.levels = fl),
       auto.key = list(
           column = 4,
           rectangles = TRUE,
           points = FALSE,
           title = "Sample size",
           cex.title = 1
       ),
       par.settings = list(
           plot.symbol = list(pch = 19, cex = 0.1)
       ),
       panel.groups = function(x, y, group.number, ...) {
           my.panel.bwplot(x, y + (group.number - 2.5) / 5, ...)
           panel.abline(v = 0, lty = 2)
       })

@

To ilustrate the orthogonality, \autoref{fig:ortho-surf} the deviance
surfaces for four simulated data set of size 1000, $\mu=5$ and different
values of the dispersion parameters. The shapes of the deviance function
show that the proposed parametrization is better for both computation
and asymptotic (normal-based) inference. Furthermore, it is interesting
to note that the deviance function shape on strong overdispersion
($\phi=-1.8$) not as well behaved as the others. This is due to the
difficulty of the distribution in dealing with strong overdispersion in
low counts (see dispersion index plot in the
\autoref{fig:indexes-plot}). This also explains the results of
Cov$(\hat{\beta}_{22}, \phi)$ in the first panel of
\autoref{fig:ortho-plot}, since $\beta_{22}$ is negative and associate
with low counts.

<<ortho-surf, fig.height=5.1, fig.width=9, fig.cap="Deviance surfaces under original and proposed parametrization for simulated data sets of size 1000 an different dispersion parameters. The ellipses are confidence regions (90, 95 and 99\\%), dotted lines are the maximum likelihood estimates, and points are the real parameters used in the simulation.">>=

#-------------------------------------------
# Plots deviance surfaces
output <- readRDS("codes/orthogonality.rds")
devs_orpar <- output$devs_orpar
devs_repar <- output$devs_repar

niveis <- c(0.9, 0.95, 0.99)
cortes <- qchisq(niveis, df = 2)
fl <- parse(text = paste("phi==", sort(phis)))

# Original parametrization
devs_orpar$fphi <- ordered(devs_orpar$realphi)
xy1 <- useOuterStrips(
    levelplot(
        deviance ~ lambda + phi | fphi + "Original parametrization",
        data = devs_orpar,
        scales = list(y = list(rot = 90, relation = "free"),
                      x = "free"),
        cuts = 30,
        xlab = expression(lambda),
        ylab = expression(phi),
        colorkey = FALSE,
        panel = function(x, y, z, at, region, ...,
                         subscripts = subscripts) {
            ##
            flambda <- devs_orpar$fitlambda[subscripts]
            fphi <- devs_orpar$fitphi[subscripts]
            ##
            tlambda <- devs_orpar$reallambda[subscripts]
            tphi <- devs_orpar$realphi[subscripts]
            ##
            panel.levelplot(x, y, z, at = at, region = TRUE,
                            ..., subscripts = subscripts)
            panel.contourplot(x, y, z, ..., at = cortes,
                              contour = TRUE, region = FALSE,
                              subscripts = subscripts)
            panel.abline(v = flambda, h = fphi, lty = 2)
            panel.points(x = tlambda, y = tphi,
                         lty = 2, pch = 19)
        }),
    strip = strip.custom(factor.levels = fl),
    strip.left = strip.custom(par.strip.text=list(cex = 0.9))
)

# Propose parametrization
devs_repar$fphi <- ordered(devs_repar$realphi)
xy2 <- useOuterStrips(
    levelplot(
        deviance ~ mu + phi2 | fphi + "Proposed parametrization",
        data = devs_repar,
        scales = list(y = list(rot = 90, relation = "free"),
                      x = "free"),
        cuts = 30,
        xlab = expression(mu),
        ylab = expression(phi),
        colorkey = FALSE,
        panel = function(x, y, z, at, region, ...,
                         subscripts = subscripts) {
            ##
            fmu <- devs_repar$fitmu[subscripts]
            fphi <- devs_repar$fitphi[subscripts]
            ##
            tmu <- devs_repar$realmu[subscripts]
            tphi <- devs_repar$realphi[subscripts]
            ##
            panel.levelplot(x, y, z, at = at, region = TRUE,
                            ..., subscripts = subscripts)
            panel.contourplot(x, y, z, ..., at = cortes,
                              contour = TRUE, region = FALSE,
                              subscripts = subscripts)
            panel.abline(v = fmu, h = fphi, lty = 2)
            panel.points(x = tmu, y = tphi,
                         lty = 2, pch = 19)
        }),
    strip = strip.custom(factor.levels = fl),
    strip.left = strip.custom(par.strip.text=list(cex = 0.9))
)

# Organize plots
print(xy1, position = c(0.0, 0.48, 1.0, 1.0), more = TRUE)
print(xy2, position = c(0.0, 0.0, 1.0, 0.52), more = FALSE)

@

\section{Case studies}
\label{case-studies}

In this section, we report three illustrative examples of count data
analysis. We considered as alternative models for the analysis the
standard Poisson regression model, the COM-Poisson model in the two
forms (original and new parametrization) and the quasi-Poisson
regression model.  The data sets and \texttt{R} code for their analysis
are available as supplementary material.

\subsection{Artificial defoliation in cotton phenology}
\label{case-cotton}

This example relates to cotton plants (\textit{Gossypium hirsutum})
submitted to five levels of artificial defoliation (\texttt{des}) and
crossed with five growth stages (\texttt{est}). The main goal of this
study was to assess the effect of defoliation levels at different growth
stages of cotton plants on the cotton production, expressed by the
number of bolls produced. The study was conducted in a greenhouse and
the experimental design was completely randomized with five
replicates. This data set was analyzed by~\citet{Zeviani2014} using the
Gamma-Count distribution.

Following~\citet{Zeviani2014}, the linear predictor is given by
$$\log(\mu_{ij}) = \beta_0 + \beta_{1j} \texttt{def}_i + \beta_{2j}
\texttt{def}_i^2$$
where $\mu_{ij}$ is the expected number of cotton bolls for the $i$-th
defoliation level ($i=$ 1: 0\%, 2: 25\%, 3: 50\%, 4: 75\% e 5: 100\%)
and $j$-th growth stage ($j$ = 1: vegetative, 2: flower bud, 3: blossom,
4: boll, 5: boll open), i.e. we have a second order effect of
defoliation in each growth stage. The parameters estimates and
goodness-of-fit measures for the Poisson, COM-Poisson, COM-Poisson$_\mu$
and quasi-Poisson regression models are presented in
\autoref{tab:coef-cotton}.

<<fit-cotton, cache=TRUE, include=FALSE>>=

#-----------------------------------------------------------------------
# Load data
# data(cottonBolls, package = "cmpreg")
cottonBolls <- read.table("./data/cottonBolls.txt",
                          header = TRUE, sep = "\t")
cottonBolls$est <- ordered(
    cottonBolls$est,
    c("vegetative", "flower bud", "blossom", "boll", "boll open")
)

#-----------------------------------------------------------------------
# Fit models
mnames <- c("PO", "C1", "C2", "QP")

# Predictor, following Zeviani et al. (2014)
form1 <- ncap ~ est:(des + I(des^2))

m1PO <- glm(form1, data = cottonBolls, family = poisson)
time11 <- system.time(
    m1C1 <- fitcm(form1, data = cottonBolls, model = "CP", sumto = 50)
)
time12 <- system.time(
    m1C2 <- fitcm(form1, data = cottonBolls, model = "CP2", sumto = 50)
)
m1QP <- glm(form1, data = cottonBolls, family = quasipoisson)

models.ncap <- list(m1PO, m1C1, m1C2, m1QP)
names(models.ncap) <- mnames

# Numbers of calls to loglik and numerical gradient
c11 <- models.ncap$C1@details$counts
c12 <- models.ncap$C2@details$counts

# LRT between Poisson and COM-Poisson (test: phi == 0)
lrt.ncap <- getAnova(m1PO, m1C2)

@

\begin{table}[!ht]
\centering \small
\caption{Parameter estimates (Est) and ratio between estimate and
  standard error (SE) for the four model strategies for the analysis
  of the cotton experiment.}
\label{tab:coef-cotton}
\begin{tabular}{lrrrrrrrr}
  \toprule
  & \multicolumn{2}{c}{Poisson} &
    \multicolumn{2}{c}{COM-Poisson} &
    \multicolumn{2}{c}{COM-Poisson$_\mu$} &
    \multicolumn{2}{c}{Quasi-Poisson} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
<<results-cotton, results="asis">>=

#-----------------------------------------------------------------------
# Goodness of fit measures and estimate parameters

# GoF measures
measures.ncap <- lapply(models.ncap, function(x)
    c("LogLik" = logLik(x), "AIC" = AIC(x), "BIC" = BIC(x)))

# Get the estimates
co1 <- coef(m1C2)
est <- lapply(models.ncap, FUN = function(x) getCoefs(x))
est.ncap <- do.call(cbind, est)

# Organize in table
pnames <- c("\\phi\\,,\\,\\sigma", "\\beta_0",
            paste0("\\beta_{1", 1:5, "}"),
            paste0("\\beta_{2", 1:5, "}"))
rownames(est.ncap) <- paste0("$", pnames, "$")
meds <- apply(do.call(cbind, measures.ncap), 1, function(x) {
    x <- formatC(x, getOption("digits"), format = "f")
    x <- gsub("NA", "-", x)
    paste(paste0("\\multicolumn{2}{c}{$", x, "$}"),
          collapse = " & ")
})
text_gof <- paste(paste(names(meds), "&", meds),
                  collapse = "\\\\\n ")

append_gof <- list(
    pos = list(nrow(est.ncap)),
    command = paste("\\specialrule{0.01em}{0.3em}{0.3em} \n",
                    text_gof, "\\\\\n",
                    "\\bottomrule"))
print.xtable(xtable(est.ncap, digits = 4,
                    label = "tab:coef-cotton"),
             hline.after = 0,
             only.contents = TRUE,
             add.to.row = append_gof)

@
\end{tabular}
\end{table}

The results presented in \autoref{tab:coef-cotton} show that the
goodness-of-fit measures (log-likelihood, AIC and BIC) are quite similar
for the COM-Poisson and COM-Poisson$_\mu$ models.  It suggests that the
reparametrization does not change the model fit, as expected.  The
Poisson model is clearly unsuitable, being overly conservative.  The
difference in terms of log-likelihood value from the Poisson to the
COM-Poisson$_\mu$ model was $\Sexpr{lrt.ncap[2, 4]}$, which in turn
suggests the better fit of the COM-Poisson$_{\mu}$ model. A chi-square
test also supports this statement.  Finally, the estimated value of the
dispersion parameter $\\hat{\phi} = \Sexpr{co1[1]}$ suggests
underdispersion.

Furthermore, results in \autoref{tab:coef-cotton} also show the
advantage of the COM-Poisson$_\mu$ model, since the estimates are quite
similar to the ones obtained by the Poisson model, whereas estimates
obtained from the COM-Poisson model in the original parametrization are
on a non interpretable scale.  The ratios between estimates and their
respective standard errors for the COM-Poisson models are very close to
ratios obtained by quasi-Poisson model. However, it is important to note
that COM-Poisson model is a full parametric approach, i.e. there is a
probability distribution associated to the counts. On the other hand,
the quasi-Poisson model is a specification based only on second-moment
assumptions.

\autoref{fig:pred-cotton} presents the observed and fitted values with
confidence intervals (95\%) as a function of the defoliation level for
each growth stage. The fitted values are the same for the Poisson and
COM-Poisson$_{\mu}$ models, however, the confidence intervals are larger
for the Poisson model because the equidispersion assumption.  The
results from the COM-Poisson$_{\mu}$ model are consistent with those
from the Gamma-Count model~\citep{Zeviani2014},
Poisson-Tweedie~\citep{Bonat2017} and the alternative parametrization of
the COM-Poisson distribution proposed by~\citet{Huang2017}.  In all
strategies the models indicated underdispersion and significant effects
of defoliation for the vegetative, blossom and boll growth stages.

<<pred-cotton, fig.height=3.5, fig.width=8.5, fig.cap="Scatterplots of the observed data and curves of fitted values with 95\\% confidence intervals as functions of the defoliation level for each growth stage.">>=

#-----------------------------------------------------------------------
# Prediction

# Data for prediction
pred <- with(cottonBolls,
             expand.grid(
                 est = levels(est),
                 des = seq(min(des), max(des), l = 20)
             ))
qn <- qnorm(0.975) * c(fit = 0, lwr = -1, upr = 1)

# Design matrix for prediction
X <- model.matrix(update(form1, NULL~.), pred)

# Considering Poisson
aux <- exp(confint(
    glht(m1PO, linfct = X), calpha = univariate_calpha())$confint)
colnames(aux) <- c("fit", "lwr", "upr")
aux <- data.frame(modelo = "Poisson", aux)
predPO.ncap <- cbind(pred, aux)

# Considering COM-Poisson
aux <- predictcm(m1C1, newdata = X)
aux <- data.frame(modelo = "COM-Poisson", aux)
predC1.ncap <- cbind(pred, aux)

# Considering COM-Poisson (mean parametrization)
aux <- predictcm(m1C2, newdata = X)
aux <- data.frame(modelo = "COM-Poisson2", aux)
predC2.ncap <- cbind(pred, aux)

# Considering Quasi-Poisson
aux <- exp(confint(
    glht(m1QP, linfct = X), calpha = univariate_calpha())$confint)
colnames(aux) <- c("fit", "lwr", "upr")
aux <- data.frame(modelo = "Quasi-Poisson", aux)
predQP.ncap <- cbind(pred, aux)

# Representing the confidence intervals
pred.ncap <- rbind(predPO.ncap, predC1.ncap, predC2.ncap, predQP.ncap)

# Legend
key <- list(columns = 4,
            cex = 0.9,
            lines = list(col = 1, lty = rev(1:4)),
            text = list(parse(
                text = c("'Poisson'", "'COM-Poisson'",
                         "'COM-Poisson'[mu]", "'Quasi-Poisson'"))
                ))

# Graph
xyplot(ncap ~ des | est,
       data = cottonBolls,
       layout = c(NA, 1),
       as.table = TRUE,
       grid = TRUE,
       type = "p",
       xlab = "Artificial defoliation level",
       ylab = "Number of bolls produced",
       spread = 0.05,
       key = key,
       alpha = 0.6,
       panel = panel.beeswarm) +
    as.layer(
        xyplot(fit ~ des | est,
               auto.key = TRUE,
               data = pred.ncap,
               groups = modelo,
               type = "l",
               layout = c(NA, 1),
               as.table = TRUE,
               col = 1,
               ly = pred.ncap$lwr,
               uy = pred.ncap$ upr,
               cty = "bands",
               fill = "gray80",
               alpha = 0.1,
               panel = panel.superpose,
               panel.groups = panel.cbH,
               prepanel = prepanel.cbH,
               lty = rev(1:4))
    )

@

In order to assess the relation between $\bm{\mu}$ and $\phi$ in the
COM-Poisson$_\mu$ parametrization, \autoref{tab:corr-cotton} presents
the empirical correlations between the regression and dispersion
parameters, as computed by the asymptotic covariance matrix of the
estimators, i.e. the inverse of the observed information.  The
correlations are practically null considering the COM-Poisson$_\mu$. On
the other hand, for the original parametrization such correlations are
quite large, in particular for the parameter $\beta_0$ (due to effects
parametrization in the linear predictor).  This result explain the
better performance of the maximization algorithm in the new
parametrization.  It is important to note that the initial values for
the \texttt{BFGS} algorithm are provided by the Poisson model, then in
the COM-Poisson$_{\mu}$ model the initial values are practically the
maximum likelihood estimates and the effort of maximization is on the
dispersion parameter $\phi$ only. To compare the computational times on
the two parametrizations we repeat the fitting $50$ times. In
this case COM-Poisson$_\mu$ fit was, on average, $38$\% faster than the
original one.

<<corr-cotton, results="asis">>=

#-----------------------------------------------------------------------
# Correlation between estimates
corr.ncap <- do.call("rbind",
                     lapply(models.ncap[c("C1", "C2")],
                            function(x) cov2cor(vcov(x))[1, -1]))

# Organize on table
rownames(corr.ncap) <- paste0("COM-Poisson", c("", "$_\\mu$"))
colnames(corr.ncap) <- gsub("beta", "hat{\\\\beta}", pnames[-1])
corrncap_aux <- divide_matrix(corr.ncap, divide = 6)

# caption <- paste("Empirical correlations between $\\hat{\\phi}$ and",
#                  "$\\hat{\\bm{\\beta}}$ for the two parametrizations",
#                  "of COM-Poisson model fit to underdispersed data.")
# print(xtable(corr.ncap,
#              align = c("lrrrrrrrrrrr"),
#              caption = caption,
#              digits = 3,
#              label = "tab:corr-cotton"),
#       size = "small",
#       sanitize.rownames.function = identity,
#       sanitize.colnames.function = function(x) sprintf("$%s$", x))


@

\begin{table}[ht]
\centering
\caption{Empirical correlations between $\hat{\phi}$ and
  $\hat{\bm{\beta}}$ for the two parametrizations of COM-Poisson model
  fit to underdispersed data.}
\label{tab:corr-cotton}
\begingroup\small
\begin{tabular}{lrrrrrr}
  \toprule
<<results="asis">>=

print(xtable(corrncap_aux[[1]], digits = 4),
      only.contents = TRUE,
      hline.after = FALSE,
      sanitize.colnames.function = function(x) sprintf("$%s$", x))
cat("\\midrule", sep = "\n")

print(xtable(corrncap_aux[[2]], digits = 4),
      only.contents = TRUE,
      sanitize.colnames.function = function(x) sprintf("$%s$", x))

@
\end{tabular}
\endgroup
\end{table}


\subsection{Soil moisture and potassium doses on soybean culture}
\label{case-soybean}

The second example is a $5\times 3$ factorial experiment in a randomized
complete block design. The aim of this study was to evaluate the effects
of potassium doses (\texttt{K}) applied to soil (0, 0.3, 0.6, 1.2 and
1.8 $\times$ 100mg dm$^{-3}$) and soil moisture (\texttt{umid}) levels
(37.5, 50, 62.5\%) on  soybean (\emph{Glicine Max}) production. The
experiment was carried out in a greenhouse, in pots with two plants, and
the count variable measured was the number of bean seeds per
pot~\citep{Serafim2012}.  \autoref{fig:desc-soy} (left) shows the number
of bean seeds recorded for each combination of potassium dose and moisture
level, it is important to note the indication of a quadratic effect of
the potassium levels as shown by smoothing curves. Most points in the
sample variance \textit{versus} sample means dispersion diagram (right)
are above the identity line, suggesting overdispersion (block effect not
yet removed).

<<fit-soy, cache=TRUE, include=FALSE>>=

#-----------------------------------------------------------------------
# Load data
# data(soyaBeans, package = "cmpreg")
soyBeans <- read.table("./data/soyaBeans.txt",
                        header = TRUE, sep = "\t")
soyBeans$umid <- as.factor(soyBeans$umid)
soyBeans <- soyBeans[-74, ] # Incorrect observation
soyBeans <- transform(soyBeans, K = K / 100)

#-----------------------------------------------------------------------
# Fit models

# Predictor
form2 <-  ngra ~ bloc + umid * K + I(K^2)

m2PO <- glm(form2, data = soyBeans, family = poisson)
time21 <- system.time(
    m2C1 <- fitcm(form2, data = soyBeans, model = "CP", sumto = 700)
)
time22 <- system.time(
    m2C2 <- fitcm(form2, data = soyBeans, model = "CP2", sumto = 700)
)
m2QP <- glm(form2, data = soyBeans, family = quasipoisson)

models.ngra <- list(m2PO, m2C1, m2C2, m2QP)
names(models.ngra) <- mnames

co2 <- coef(m2C2)

# Numbers of calls to loglik and numerical gradient
c21 <- models.ngra$C1@details$counts
c22 <- models.ngra$C2@details$counts

# # Profile extra parameter
# profs.ngra <- lapply(list(c(m2C1, "phi"), c(m2C2, "phi2")),
#                      function(x) myprofile(x[[1]], x[[2]]))
# profs.ngra <- do.call("rbind", profs.ngra)

# LRT between Poisson and COM-Poisson (test: phi == 0)
lrt.ngra <- getAnova(m2PO, m2C2)

@

<<desc-soy, fig.pos="!ht", fig.height=4, fig.width=8, fig.cap="Number of bean seeds per pot for each potassium dose and moisture level (left) and sample mean against sample variance of the five replicates for each experimental treatment (right). Solid lines are the smoothing curves on the left and the least of squares curve on the right.">>=

#-----------------------------------------------------------------------
# Exploratory analysis

# Scatter plot
xy1 <- xyplot(ngra ~ K | umid,
              data = soyBeans,
              xlab = "Potassium fertilization level",
              ylab = "Number of grains per pot",
              type = c("p", "g", "smooth"),
              as.table =  TRUE,
              layout = c(2, 2),
              strip = strip.custom(
                  strip.names = TRUE, var.name = "moisture",
                  factor.levels = paste0(levels(soyBeans$umid), "%")))

# Sample variance vs sample mean (evidence in favor of the
# overdispersion).
mv <- soyBeans %>%
    group_by(K, umid) %>%
    summarise(mu = mean(ngra), va = var(ngra))
xlim <- ylim <- extendrange(c(mv$mu, mv$va), f = 0.05)

xy2 <- xyplot(va ~ mu,
              data = mv,
              type = c("p", "r", "g"),
              xlim = xlim,
              ylim = ylim,
              xlab = expression("Sample"~"mean"~(bar(y))),
              ylab = expression("Sample"~"variance"~(s^2)),
              panel = function(...) {
                  panel.xyplot(...)
                  panel.abline(a = 0, b = 1, lty = 2)
              })

print(xy1, split = c(1, 1, 2, 1), more = TRUE)
print(xy2, split = c(2, 1, 2, 1), more = FALSE)

@

For the analysis of this data set based on the descriptive analysis
(\autoref{fig:desc-soy}), we proposed the following linear predictor
$$
\log(\mu_{ijk}) = \beta_0 + \gamma_i + \tau_j +
  \beta_{1}\texttt{K}_k + \beta_{2}\texttt{K}_k^2 +
  \beta_{3j}\texttt{K}_k
$$
where $i=$1: block II, 2: block III, 3: block IV e 4: block V; $j=$1:
50\% e 2: 62.5\%; and $k=$1: 0.0, 2: 0.3, 3: 0.6, 4: 1.2, 5: 1.8 100mg
dm$^{-3}$, where $\gamma_i$ is the effect of $i$-th block ($i=$1: block
II, 2: block III, 3: block IV and 4: block V), $\tau_j$ is the effect of
$j$-th moisture level ($j=$1: 50\% and 2: 62.5\%) and $\beta_{3j}$ is
interaction of the first order potassium effect (\texttt{K}) for the
$j$-th moisture level (\texttt{umid}). \autoref{tab:coef-soy} presents
the estimates, ratio between estimate and standard error and
goodness-of-fit measures for the alternative models.

The results in \autoref{tab:coef-soy} show that the two parametrization
of COM-Poisson model presented very similar goodness-of-fit measures and
better fit than the Poisson model. The difference between the
log-likelihoods of the Poisson and COM-Poisson models was
$\Sexpr{lrt.ngra[2, 4]}$, indicating that $\phi$ is significantly
different from zero. The estimate of $\phi$ ($\Sexpr{co2[1]}$) indicates
overdispersion, corroborating the descriptive analysis. Concerning to
the regression parameters, the similarities between the models are
analogous to the previous section. Both models indicate effects of
block, potassium dose and moisture level, however the Poisson model
indicates the effects with greater significance, because it does not
take account of the extra variability.

\begin{table}[!ht]
\centering \small
\caption{Parameter estimates (Est) and ratio between estimate and
  standard error (SE) for the four model strategies for the analysis of
  the soybean experiment.}
\label{tab:coef-soy}
\begin{tabular}{lrrrrrrrr}
  \toprule
  & \multicolumn{2}{c}{Poisson} &
    \multicolumn{2}{c}{COM-Poisson} &
    \multicolumn{2}{c}{COM-Poisson$_\mu$} &
    \multicolumn{2}{c}{Quasi-Poisson} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
<<results-soya, results="asis">>=

#-----------------------------------------------------------------------
# Goodness of fit measures and estimate parameters

# GoF measures
measures.ngra <- lapply(models.ngra, function(x)
    c("LogLik" = logLik(x), "AIC" = AIC(x), "BIC" = BIC(x)))

# Get the estimates
co2 <- coef(m2C2)
est <- lapply(models.ngra, FUN = function(x) getCoefs(x))
est.ngra <- do.call(cbind, est)

# Organize in table
pnames <- c("\\phi\\,,\\,\\sigma", "\\beta_0",
            paste0("\\gamma_{", 1:4, "}"),
            paste0("\\tau_{", 1:2, "}"),
            "\\beta_1", "\\beta_2",
            paste0("\\beta_{3", 1:2, "}"))

rownames(est.ngra) <- paste0("$", pnames, "$")
meds <- apply(do.call(cbind, measures.ngra), 1, function(x) {
    x <- formatC(x, getOption("digits"), format = "f")
    x <- gsub("NA", "-", x)
    paste(paste0("\\multicolumn{2}{c}{$", x, "$}"),
          collapse = " & ")
})
text_gof <- paste(paste(names(meds), "&", meds),
                  collapse = "\\\\\n ")

append_gof <- list(
    pos = list(nrow(est.ngra)),
    command = paste("\\specialrule{0.01em}{0.3em}{0.3em} \n",
                    text_gof, "\\\\\n",
                    "\\bottomrule"))
print.xtable(xtable(est.ngra, digits = 4,
                    label = "tab:coef-soy"),
             hline.after = 0,
             only.contents = TRUE,
             add.to.row = append_gof)

@
\end{tabular}
\end{table}

The infinite sum $Z(\mu, \phi)$ in the cases of overdispersed count data
requires a larger upper bound to reach convergence. Thus, in these cases
the computation of the log-likelihood function is computationally
expensive.  For the data set considered, the upper bound was fixed at
$700$.  The \texttt{BFGS} algorithm evaluated the log-likelihood
function $\Sexpr{c21[1]}$ and $\Sexpr{c22[2]}$ times to reach
convergence, when using the original and new parametrization of the
COM-Poisson distribution, respectively.  In terms of computational time,
for 50 repetitions of fit, the proposed reparametrization was on average
$110\%$ faster than the original one.  Probably, it is due to the better
behaviour of the log-likelihood function as well as better initial
values obtained from the Poisson fit. In \autoref{tab:corr-soy}, we
present the empirical correlation between the regression and dispersion
parameter estimates. The correlations are close to zero for the
COM-Poisson$_\mu$ model, indicating the empirical orthogonality between
$\mu$ and $\phi$.

<<corr-soy, results="asis">>=

##----------------------------------------------------------------------
## Correlation between estimates
corr.ngra <- do.call("rbind",
                     lapply(models.ngra[c("C1", "C2")],
                            function(x) cov2cor(vcov(x))[1, -1]))

## Organize on table
rownames(corr.ngra) <- paste0("COM-Poisson", c("", "$_\\mu$"))
colnames(corr.ngra) <- gsub("(beta|tau|gamma)", "hat{\\\\\\1}",
                            pnames[-1])
corrngra_aux <- divide_matrix(corr.ncap, divide = 7)

# caption <- paste("Empirical correlations between $\\hat{\\phi}$ and",
#                  "$\\hat{\\bm{\\beta}}$ for the two parametrizations",
#                  "of COM-Poisson model fit to overdispersed data.")
# print(xtable(corr.ngra,
#              align = c("lccccccccccc"),
#              caption = caption,
#              digits = 3,
#              label = "tab:corr-soy"),
#       size = "small",
#       sanitize.rownames.function = identity,
#       sanitize.colnames.function = function(x) sprintf("$%s$", x))

@

\begin{table}[ht]
\centering
\caption{Empirical correlations between $\hat{\phi}$ and
  $\hat{\bm{\beta}}$ for the two parametrizations of COM-Poisson model
  fit to overdispersed data.}
\label{tab:corr-soy}
\begingroup\small
\begin{tabular}{lrrrrrrr}
  \toprule
<<results="asis">>=

print(xtable(corrngra_aux[[1]], digits = 4),
      only.contents = TRUE,
      hline.after = FALSE,
      sanitize.colnames.function = function(x) sprintf("$%s$", x))
cat("\\midrule", sep = "\n")

print(xtable(corrngra_aux[[2]], digits = 4),
      only.contents = TRUE,
      sanitize.colnames.function = function(x) sprintf("$%s$", x))

@
\end{tabular}
\endgroup
\end{table}

The observed and fitted counts for each humidity level with confidence
intervals are shown in \autoref{fig:pred-soy}. The fitted values are
identical for the Poisson and COM-Poisson$_{\mu}$ models, leading to the
same conclusions. On the other hand, confidence intervals for the
Poisson model are smaller than the ones from the COM-Poisson$_{\mu}$,
due to the equidispersion assumption underlying the Poisson model. The
confidence intervals from the COM-Poisson$_{\mu}$ and quasi-Poisson
models are really similar, which in turn shows the already highlighted
similarity between these approaches, however only the COM-Poisson
model$_{\mu}$ corresponds to a fully specified probability model.

<<pred-soy, fig.pos="!ht", fig.height=3.8, fig.width=8, fig.cap="Dispersion diagrams of been seeds counts as function of potassium doses and humidity levels with fitted curves and confidence intervals (95\\%).">>=

#-----------------------------------------------------------------------
# Prediction

# Data for prediction
pred <- with(soyBeans,
             expand.grid(
                 bloc = factor(levels(bloc)[1], levels = levels(bloc)),
                 umid = levels(umid),
                 K = seq(min(K), max(K), l = 20)
             ))
qn <- qnorm(0.975) * c(fit = 0, lwr = -1, upr = 1)

# Design matrix for prediction
X <- model.matrix(update(form2, NULL ~ .), pred)
bl <- attr(X, "assign") == 1
X[, bl] <- X[, bl] + 1/(sum(bl) + 1)

# Considering Poisson
aux <- exp(confint(
    glht(m2PO, linfct = X), calpha = univariate_calpha())$confint)
colnames(aux) <- c("fit", "lwr", "upr")
aux <- data.frame(modelo = "Poisson", aux)
predPO.ngra <- cbind(pred, aux)

# Considering COM-Poisson
aux <- predictcm(m2C1, newdata = X)
aux <- data.frame(modelo = "COM-Poisson", aux)
predC1.ngra <- cbind(pred, aux)

# Considering COM-Poisson (mean parametrization)
aux <- predictcm(m2C2, newdata = X)
aux <- data.frame(modelo = "COM-Poisson2", aux)
predC2.ngra <- cbind(pred, aux)

# Considering Quasi-Poisson
aux <- exp(confint(
    glht(m2QP, linfct = X), calpha = univariate_calpha())$confint)
colnames(aux) <- c("fit", "lwr", "upr")
aux <- data.frame(modelo = "Quasi-Poisson", aux)
predQP.ngra <- cbind(pred, aux)

# Representing the confidence intervals
pred.ngra <- rbind(predPO.ngra, predC1.ngra, predC2.ngra, predQP.ngra)

# Legend
key <- list(columns = 4,
            cex = 0.9,
            lines = list(col = 1, lty = rev(1:4)),
            text = list(parse(
                text = c("'Poisson'", "'COM-Poisson'",
                         "'COM-Poisson'[mu]", "'Quasi-Poisson'"))
                )
            )

# Graph
update(xy1, layout = c(NA, 1), type = c("p", "g"),
       alpha = 0.6, key = key) +
    as.layer(
        xyplot(fit ~ K | umid,
               data = pred.ngra,
               groups = modelo,
               type = "l",
               col = 1,
               ly = pred.ngra$lwr,
               uy = pred.ngra$upr,
               cty = "bands",
               fill = "gray80",
               alpha = 0.1,
               panel = panel.superpose,
               panel.groups = panel.cbH,
               prepanel = cmpreg::prepanel.cbH,
               lty = rev(1:4))
    )

@

\subsection{Assessing toxicity of nitrofen in aquatic systems}

Nitrofen is a herbicide that was used extensively for the control of
broad-leaved and grass weeds in cereals and rice. Although it is
relatively non-toxic to adult mammals, nitrofen is a significant
tetragen and mutagen. It is also acutely toxic and reproductively toxic
to cladoceran zooplankton. Nitrofen is no longer in commercial use in
the U.S., having been the first pesticide to be withdrawn due to
tetragenic effects~\citep{Bailer1994}.

The data set comes from an experiment to measure the reproductive
toxicity of the herbicide, nitrofen, on a species of zooplankton
(\textit{Ceriodaphnia dubia}). Fifty animals were randomized into
batches of ten and each batch was put in a solution with a measured
concentration of nitrofen ($0, 0.8, 1.6, 2.35$ and $3.10$
$\mu$g$/10^2$litre) (\texttt{dose}). Then the number of live offspring
was recorded.

<<fit-ovos, cahe=TRUE, include=FALSE>>=

#-----------------------------------------------------------------------
# Load data
# data(nitrofen, package = "boot")
# data(Paula, package = "labestData")
# nitrofen <- PaulaEx4.6.20
nitrofen <- read.table("./data/nitrofen.txt",
                       header = TRUE, sep = "\t")
nitrofen <- transform(nitrofen, dose = dose / 100)

#-----------------------------------------------------------------------
# Fit models
mnames <- c("PO", "C1", "C2", "QP")

# Predictors
form31 <-  novos ~ dose
form32 <-  novos ~ dose + I(dose^2)
form33 <-  novos ~ dose + I(dose^2) + I(dose^3)

predictors <- list("pred1" = form31, "pred2" = form32, "pred3" = form33)
fmodels.ovos <- lapply(predictors, function(form) {
    PO <- glm(form, data = nitrofen, family = poisson)
    C1 <- fitcm(form, data = nitrofen, model = "CP", sumto = 100)
    C2 <- fitcm(form, data = nitrofen, model = "CP2", sumto = 100)
    QP <- glm(form, data = nitrofen, family = quasipoisson)
    list("PO" = PO, "C1" = C1, "C2" = C2, "QP" = QP)
})

#-----------------------------------------------------------------------
# LRT for nested models

# Poisson
auxPO <- lapply(fmodels.ovos, function(x) x$PO)
do.call("getAnova", auxPO)

# COM-Poisson standard
auxC1 <- lapply(fmodels.ovos, function(x) x$C1)
do.call("getAnova", auxC1)

# COM-Poisson mean-parameterized
auxC2 <- lapply(fmodels.ovos, function(x) x$C2)
do.call("getAnova", auxC2)

# Quasi-Poisson
auxQP <- lapply(fmodels.ovos, function(x) x$QP)
do.call("getAnova", auxQP)

#--------------------------------------------
# Separe the choose models
form3 <- form33
m3PO <- fmodels.ovos$pred3$PO
m3C1 <- fmodels.ovos$pred3$C1
m3C2 <- fmodels.ovos$pred3$C2
m3QP <- fmodels.ovos$pred3$QP

models.ovos <- list(m3PO, m3C1, m3C2, m3QP)
names(models.ovos) <- mnames

# Numbers of calls to loglik and numerical gradient
models.ovos$C1@details$counts
models.ovos$C2@details$counts

@

For this data set we consider three models with linear predictors,
\begin{center}
\begin{minipage}{12cm}
Linear: $\log(\mu_i) = \beta_0 + \beta_1 \texttt{dose}_i$\\
Quadratic: $\log(\mu_i) = \beta_0 + \beta_1 \texttt{dose}_i +
             \beta_2 \texttt{dose}_i^2$\\
Cubic: $\log(\mu_i) = \beta_0 + \beta_1 \texttt{dose}_i +
             \beta_2 \texttt{dose}_i^2 + \beta_3 \texttt{dose}_i^3$.
\end{minipage}
\end{center}

\begin{table}[!ht]
\centering \small
\caption{Model fit measures and comparisons between predictors and
  models fitted to the nitrofen data.}
\label{tab:anova-ovos}
\begin{tabularx}{\textwidth}{lCCCCCrC}
  \toprule
 Poisson & np & $\ell$ & AIC & 2(diff $\ell$) & diff np & P($>\rchi^2$) & \\
 \midrule
<<anova-ovos1, results="asis">>=

auxPO <- lapply(fmodels.ovos, function(x) x$PO)
tab <- do.call("getAnova", c(print = FALSE, auxPO))
tab <- cbind(tab, NA)
rownames(tab) <- paste("Preditor", rownames(tab))
digits <- c(1, 0, 3, 3, 3, 0, -2, 3)
print(xtable(tab, digits = digits),
      include.colnames = FALSE,
      hline.after = NULL,
      only.contents = TRUE)

@
\specialrule{0em}{0.5em}{0em} %% Apenas para espaçamento
  COM-Poisson & np & $\ell$ & AIC & 2(diff $\ell$) & diff np &
  P($>\rchi^2$) & $\hat{\phi}$ \\
  \midrule
<<anova-ovos2, results="asis">>=

auxC1 <- lapply(fmodels.ovos, function(x) x$C1)
tab <- do.call("getAnova", c(print = FALSE, auxC1))
tab <- cbind(tab, sapply(auxC1, function(x) coef(x)[1]))
rownames(tab) <- paste("Preditor", rownames(tab))
print(xtable(tab, digits = digits),
      include.colnames = FALSE,
      hline.after = NULL,
      only.contents = TRUE)

@
\specialrule{0em}{0.5em}{0em} %% Apenas para espaçamento
  COM-Poisson$_\mu$ & np & $\ell$ & AIC & 2(diff $\ell$) & diff np &
  P($>\rchi^2$) & $\hat{\phi}$ \\
  \midrule
<<anova-ovos3, results="asis">>=

auxC2 <- lapply(fmodels.ovos, function(x) x$C2)
tab <- do.call("getAnova", c(print = FALSE, auxC2))
tab <- cbind(tab, sapply(auxC2, function(x) coef(x)[1]))
rownames(tab) <- paste("Preditor", rownames(tab))
print(xtable(tab, digits = digits),
      include.colnames = FALSE,
      hline.after = NULL,
      only.contents = TRUE)

@
\specialrule{0em}{0.5em}{0em} %% Apenas para espaçamento
  Quasi-Poisson & np & QDev & AIC & F & diff np & P($>F$) & $\hat{\sigma}$ \\
  \midrule
<<anova-ovos4, results="asis">>=

auxQP <- lapply(fmodels.ovos, function(x) x$QP)
tab <- do.call("getAnova", c(print = FALSE, auxQP))
tab <- cbind(tab, sapply(auxQP, function(x) summary(x)$dispersion))
rownames(tab) <- paste("Preditor", rownames(tab))
print(xtable(tab, digits = digits),
      include.colnames = FALSE,
      hline.after = NULL,
      only.contents = TRUE)

@
 \bottomrule
\end{tabularx}
%\vspace{-1mm}

\footnotesize \raggedright np, number of parameters; diff $\ell$,
difference in log-likelihoods; QDev, quasi-deviance, F, F statistics
based on quasi-deviances; diff np, difference in np.
\end{table}

\autoref{tab:anova-ovos} summarizes the results of the fitted models and
likelihood ratio tests comparing the sequence of predictors. All models
indicate the significance of the cubic effect of the nitrofen
concentration.  Considering this predictor, there is an evidence of
equidispersed counts, the $\phi$ estimate of the COM-Poisson is close to
zero and $\sigma$ of quasi-Poisson is close to one.  It is interesting
to note that if we omit the high order effects the models show evidence
of overdispersion. This exemplifies the discussion on the causes of
overdispersion made in \autoref{introduction}.  We can also note that
the quasi-Poisson approach, although robust to equidispersion
assumption, shows higher descriptive levels ($p$-values) than parametric
models, that is, the tests under parametric models are more powerful
than the ones under the quasi-Poisson model in the equidispersed case.

In \autoref{tab:coef-ovos}, we present the estimates of the regression
parameters considering the cubic dose model. The interpretations are
similar to others cases studies, however, in this case the Poisson model
is also suitable for indicating the significance of the covariate
effects.  In addition, note that the parameter estimates of the original
COM-Poisson model are comparable to the others models. This occurs
because we are in the particular case, where $\phi = 0$, which implies
$\lambda = \mu$.

\begin{table}[!ht]
\centering
\caption{Parameter estimates (Est) and ratio between estimate and
  standard error (SE) for the four model strategies for the analysis of
  the nitrofen experiment.}
\label{tab:coef-ovos}
\begin{tabular}{lrrrrrrrr}
  \toprule
  & \multicolumn{2}{c}{Poisson} &
    \multicolumn{2}{c}{COM-Poisson} &
    \multicolumn{2}{c}{COM-Poisson$_\mu$} &
    \multicolumn{2}{c}{Quasi-Poisson} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
<<coef-ovos, results="asis">>=

#-----------------------------------------------------------------------
# Goodness of fit measures and estimate parameters

# GoF measures
measures.ovos <- lapply(models.ovos, function(x)
    c("LogLik" = logLik(x), "AIC" = AIC(x), "BIC" = BIC(x)))

# Get the estimates
co2 <- coef(m2C2)
est <- lapply(models.ovos, FUN = function(x) getCoefs(x))
est.ovos <- do.call(cbind, est)[-1, ]

# Organize in table
pnames <- paste0("\\beta_{", 0:3, "}")
rownames(est.ovos) <- paste0("$", pnames, "$")
print.xtable(xtable(est.ovos, digits = 4),
             hline.after = c(0, nrow(est.ovos)),
             only.contents = TRUE)

@
\end{tabular}
\end{table}

\autoref{fig:pred-ovos} shows the number of live off-spring observed and
fitted curves along with confidence intervals for all model strategies
adopted.  The fitted values and confidence intervals are identical and
have a complete overlap. It shows that the estimation of the extra
dispersion parameter does not affect the estimation of the regression
coefficients in the case of equidispersed counts.

<<pred-ovos, fig.height=3.5, fig.width=5.5, out.width="0.7\\textwidth", fig.cap="Number of live offsprings observed for each nitrofen concentration level with fitted curves and 95\\% confidence intervals.">>=

#-----------------------------------------------------------------------
# Prediction

# Data for prediction
pred <- with(nitrofen,
             data.frame("dose" = seq(min(dose), max(dose),
                                     length.out = 100)))
qn <- qnorm(0.975) * c(fit = 0, lwr = -1, upr = 1)

# Design matrix for prediction
X <- model.matrix(update(form3, NULL ~ .), pred)

# Considering Poisson
aux <- exp(confint(
    glht(m3PO, linfct = X), calpha = univariate_calpha())$confint)
colnames(aux) <- c("fit", "lwr", "upr")
aux <- data.frame(modelo = "Poisson", aux)
predPO.novos <- cbind(pred, aux)

# Considering COM-Poisson
aux <- predictcm(m3C1, newdata = X)
aux <- data.frame(modelo = "COM-Poisson", aux)
predC1.novos <- cbind(pred, aux)

# Considering COM-Poisson (mean parametrization)
aux <- predictcm(m3C2, newdata = X)
aux <- data.frame(modelo = "COM-Poisson2", aux)
predC2.novos <- cbind(pred, aux)

# Considering Quasi-Poisson
aux <- exp(confint(
    glht(m3QP, linfct = X), calpha = univariate_calpha())$confint)
colnames(aux) <- c("fit", "lwr", "upr")
aux <- data.frame(modelo = "Quasi-Poisson", aux)
predQP.novos <- cbind(pred, aux)

# Representing the confidence intervals
pred.novos <- rbind(predPO.novos, predC1.novos, predC2.novos, predQP.novos)
ord <- order(pred.novos$dose, pred.novos$modelo)
pred.novos <- pred.novos[ord, ]

# Legend
key <- list(columns = 2,
            lines = list(col = 1, lty = rev(1:4)),
            text = list(parse(
                text = c("'Poisson'", "'COM-Poisson'",
                         "'COM-Poisson'[mu]", "'Quasi-Poisson'"))
                )
            )

# Graph
xyplot(novos ~ dose,
       data = nitrofen,
       xlab = "Nitrofen concentration level",
       ylab = "Number of live offspring",
       grid = TRUE,
       alpha = 0.6,
       key = key,
       spread = 0.05,
       panel = panel.beeswarm) +
    as.layer(
        xyplot(fit ~ dose,
               auto.key = TRUE,
               data = pred.novos,
               groups = modelo,
               type = "l",
               col = 1,
               ly = pred.novos$lwr,
               uy = pred.novos$upr,
               cty = "bands",
               fill = "gray80",
               alpha = 0.1,
               panel = panel.superpose,
               panel.groups = panel.cbH,
               prepanel = prepanel.cbH,
               lty = rev(1:4))
    )

@

Finally, in \autoref{tab:corr-ovos} we present the empirical
correlations between the regression and dispersion parameter estimates.
The results show that even in the special case ($\phi=0$), the empirical
correlations for the original COM-Poisson model are not zero.  For the
reparametrized model, as discussed in the previous sections, the
correlations are practically null. The computational times for fifty
repetitions of fit are similar. The average time to fit for the
COM-Poisson$_\mu$ and COM-Poisson models is 1.19 and 1.09 seconds,
respectively.

<<corr-ovos, results="asis">>=

#-----------------------------------------------------------------------
# Correlation between estimates
corr.ovos <- do.call("rbind",
                     lapply(models.ovos[c("C1", "C2")],
                            function(x) cov2cor(vcov(x))[1, -1]))

# Organize on table
rownames(corr.ovos) <- paste0("COM-Poisson", c("", "$_\\mu$"))
colnames(corr.ovos) <- gsub("(beta)", "hat{\\\\\\1}", pnames)

caption <- paste("Empirical correlations between $\\hat{\\phi}$ and",
                 "$\\hat{\\bm{\\beta}}$ for the two parametrizations",
                 "of COM-Poisson model fit to equidispersed data.")
print(xtable(corr.ovos,
             align = c("lrrrr"),
             caption = caption,
             digits = 4,
             label = "tab:corr-ovos"),
      # size = "small",
      sanitize.rownames.function = identity,
      sanitize.colnames.function = function(x) sprintf("$%s$", x),
      # width = "\\textwidth,"
      tabular.environment = "tabular")

@

\section{Concluding remarks}
\label{conclusion}

In this paper, we presented a novel parametrization of the COM-Poisson
distribution and the associated regression model. The novel
parametrization was based on a simple asymptotic approximation for the
expectation and variance of the COM-Poisson distribution. The main
advantage of the proposed reparametrization is the simple interpretation
of the regression coefficients in terms of the expectation of the
response variable as usual in the generalized linear models
context. Thus, it is possible to compare the results of the COM-Poisson
model with the ones from standard approaches as the Poisson and
quasi-Poisson regression models. Furthermore, in the novel
parametrization the COM-Poisson distribution is indexed by the
expectation $\mu$ and an extra dispersion parameter $\phi$ which our
data analysis suggest to be orthogonal.

We evaluated the accuracy of the asymptotic approximations for the
expectation and variance of the COM-Poisson distribution by considering
quadratic approximation errors.  The results showed that the
approximations are accurate for a large part of the parameter space,
which in turn support our reparametrization.  We carried out a
simulation study to assess the properties of the reparametrized
COM-Poisson model to deal with different levels of dispersion as well as
the properties of the maximum likelihood estimators. The results of our
simulation study suggested that the maximum likelihood estimators of the
regression and dispersion parameters are unbiased and consistent.  The
empirical coverage rates of the confidence intervals computed based on
the asymptotic distribution of the maximum likelihood estimators are
close to the nominal level for sample size greater than $100$.  The
worst scenario is when we have small sample sizes and strong
overdispersed counts.  In general, we recommend the use of the
asymptotic confidence intervals for computational simplicity.

The data analyses have shown that the COM-Poisson regression model is a
suitable choice to deal with dispersed count data.  The observed
empirical correlation between the regression and dispersion parameter
estimators suggest orthogonality between $\mu$ and $\phi$ in
COM-Poisson$_\mu$ distribution. Thus, the computational procedure based
on the proposed reparametrization is faster than in the original
parametrization.

In general, the results presented by the reparametrized COM-Poisson
models were satisfactory and comparable to the conventional approaches.
Therefore, its use in the analysis of count data is encouraged. The
computational routines for fitting the original and reparametrized
COM-Poisson regression models are available in the supplementary
material\textsuperscript{\ref{papercompanion}}.

There are many possible extensions to the model discussed in this paper,
including simulation studies to assess the model robustness against
model misspecification and to assess the theoretical approximations for
$Z(\lambda, \nu)$ (or $Z(\mu,\phi))$. Another simple extension of the
proposed model is to model both $\mu$ and $\phi$ parameters as functions
of covariate in a double generalized linear models framework.  Finally,
the reparametrized version of the COM-Poisson model also encourages the
specification of generalized linear mixed models using this
distribution.

%-----------------------------------------------------------------------
\bibliography{references.bib}

\end{document}
