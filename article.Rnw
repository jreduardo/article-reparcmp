\documentclass[a4paper]{article}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bm}
\DeclareRobustCommand{\rchi}{{\mathpalette\irchi\relax}}
\newcommand{\irchi}[2]{\raisebox{\depth}{$#1\chi$}}

\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue,
  urlcolor=blue}

\usepackage[bottom]{footmisc}
\usepackage{geometry}
\geometry{lmargin=3cm, rmargin=3cm, tmargin=3cm, bmargin=3cm}

% Algorithm
\usepackage[english, vlined, ruled]{algorithm2e}
\SetAlCapSty{}
% \SetAlCapFnt{\footnotesize}
% \SetAlCapNameFnt{\footnotesize}

\usepackage{natbib}
% \bibliographystyle{humannat}
\bibliographystyle{agsm}

\usepackage{lipsum}
\usepackage{float}
\usepackage{mathpazo}

\usepackage{booktabs}
\usepackage{tabularx}
\newcolumntype{C}{>{\centering\arraybackslash}X}

\makeatletter
\def\and{%
  \end{tabular}%
  \hskip 0em \@plus.17fil \protect$\bullet$
  \begin{tabular}[t]{c}}%
\makeatother

%-----------------------------------------------------------------------
\title{Reparametrization of COM-Poisson Regression Models with
  Applications in the Analysis of Experimental Data}

\author{
  Eduardo E. R. Junior\thanks{Corresponding author:
    \href{mailto:jreduardo@usp.br}{\nolinkurl{jreduardo@usp.br}}}
    \hspace*{.8mm}\thanks{Department of Exact Sciences, ESALQ-USP,
    Piracicaba, S\~{a}o Paulo, Brazil} \and
  Walmes M. Zeviani\thanks{Department of Statistics, UFPR, Curitiba,
    Paran\'{a}, Brazil} \and
  Wagner H. Bonat\footnotemark[3] \and
  Clarice G. B. Dem\'{e}trio\footnotemark[2]
}

\date{\today}

<<setup, include=FALSE>>=

library(knitr)
library(xtable)
options(digits = 3, OutDec = ".",
        xtable.caption.placement = "top",
        xtable.booktabs = TRUE,
        xtable.sanitize.text.function = identity)
opts_chunk$set(
    warning = FALSE,
    message = FALSE,
    echo = FALSE,
    results = "hide",
    fig.width = 7,
    fig.height = 5,
    out.width = "1\\textwidth",
    fig.align = "center",
    fig.pos = "H",
    # dev = "tikz"
    dev.args = list(family = "Palatino")
    )

source("./codes/lattice-panels.R")
source("./codes/functions.R")

# Colors for legends
cols <- trellis.par.get("superpose.line")$col

# Useful packages
library(bbmle)
library(multcomp)
library(plyr)
library(tidyr)
library(dplyr)

@

%-----------------------------------------------------------------------

\begin{document}
\maketitle

\begin{abstract}

  In the analysis of count data seldom the equidispersion assumption is
  suitable, hence the Poisson regression model is inappropriate in many
  situations. The COM-Poisson is a member of the exponential family, has
  the Poisson and geometric distributions as special cases and the
  Bernoulli distribution as a limiting case. It can deal with both under
  and overdispersed count data. Therefore it is a suitable distribution
  to the analysis of dispersed count data. In spite of the nice
  properties of COM-Poisson, the location parameter of the distribution
  does not represent the conditional mean making difficult
  interpretation in a regression structure. In this paper we present a
  very simple reparametrization based on mean approximation. The
  estimation and inference is based on likelihood and simulation study
  showed that the maximum likelihood estimators are unbiased and
  consistent for both regression and precision parameters. Furthermore,
  we observed empirical correlation between regression and precision
  parameters close to zero suggesting the orthogonality feature in the
  proposed mean reparametrized model. We present applications of the
  proposed model through three case studies with over, under and
  equidispersed counts. Results show improvements over the conventional
  approaches. Therefore, its use in the analysis of count data is
  encouraged. The computational routines for fitting of original and
  reparametrized COM-Poisson models are available in the supplementary
  material.

\vspace{0.1cm}
\noindent\textbf{keywords:} COM-Poisson, Over/Under-dispersion, Count
data, Likelihood inference.
\end{abstract}

\pagebreak

\section{Introduction}
\label{introduction}

Count data are variables that assume non-negative integer values and
represent the number of times an event occurs in the observation
unit. This kind of data is common in experimental research such as
the number of grains produced by a plant, number of fruits produced by a
tree, number of insects captured by a trap, among others. Since the
seminal paper by \citet{Nelder1972} introduced the class of generalized
linear models (GLM), analysis of count data often consider the Poisson
regression model. This model provides a suitable distribution for a
counting random variable and an efficient Newton scoring algorithm is
used to fit it. Furthermore, the theoretical background for GLMs is
well established in the class of dispersion models \citep{Jorgensen1997}
as a generalization of the exponential family of distributions.

In spite of the advantages of the GLM framework, the Poisson
distribution has only one parameter, which represents the mean, and
also, the variance of the count random variable. This relationship is
referred as equidispersion.  However, this assumption can be unsuitable
to many situations and when the Poisson model is applied in these cases
the standard errors for the parameters estimates are inconsistent
\citep{Winkelmann1995}.

In practice, overdispersion, when the variance greater than the mean, is
largely reported in the literature and may occur due to the absence of
relevant covariates, heterogeneity of sampling units, different domain
size not considered and excess of zero \citep{Hinde1998}. Lesser
reported, but that has received attention from the statistical
community, is a underdispersion case, when the variance is less than the
mean. The processes that reduce the variability are not as well known as
those lead to extra variability. For the same reason, there are a few
approaches to deal underdispersed cases. The explanatory mechanisms to
underdispersion may be related to Poisson process. When the time between
events is not exponential distributed, the number of events can be over
or underdispersed, this motivated the class of duration dependence
models \citep{Winkelmann1995}. Other possibility to explain
underdispersion are order statistics. Suppose $Y_i$ is Poisson
distributed then maximum of $Y_i$ is underdispersed.

The strategies for constructing alternative count distributions are
related with the causes of the non-equidispersion. Particular to
overdispersion case Poisson mixed models are widely applied. One popular
example of this approach is the negative-binomial model, where the mean
is added a random noise that is assumed to follow a gamma
distribution. However, other distributions for the random effects may be
considered, for example the model Poisson-Tweedie \citep{Bonat2017} and
Poisson-Inverse-Gaussian. For underdispersion, the Gamma-Count
distribution assumes a gamma distribution for the time between events,
thus it can deal with dispersed counts \citep{Zeviani2014}. The
COM-Poisson distribution is given by a generalization the Poisson
distribution allowed for a non-linear decrease in ratios of successive
probabilities \citep{Shmueli2005}.

The COM-Poisson is a member of the exponential family, has the Poisson
and geometric distributions as special cases and the Bernoulli
distribution as a limiting case. It can deal with both under and
overdispersed count data. Some recently applications of the COM-Poisson
distribution are related by \citet{Lord2010} for analysis of traffic
crash data, \citet{Sellers2010} to model airfreight breakage and book
purchases and \citet{Huang2017} with analysis of attendance data,
takeover birds and cotton bolls count. The main disadvantage of
COM-Poisson distribution is that the location parameter does not
represent the regression conditional mean making difficult
interpretation in a regression structure. \citet{Huang2017} proposed a
mean-parametrization COM-Poisson distribution to avoid this issue, in
this approach the mean parameter is obtained by solving the root of an
equation defined as a infinite sum.

The main goal of this article is to proposed a novel COM-Poisson
parametrization based on mean approximation given by
\citet{Shmueli2005}. In this parametrization the probability mass
function is written with $\mu$ and $\phi$ as function of original
parameters, where $\mu$ is the mean and $\phi$ is the precision or
dispersion parameter. Opposite the original parametrization, the
proposed parametrization leads to interpretability of the parameters in
a regression model and it is more simple than mean-parametrization gives
by \cite{Huang2017} because in this case it is not necessary find $\mu$
as the root of an equation involving an infinite sum. We conduce a
simulation study to performe the properties of the maximum likelihood
estimators for the proposed mean-parametrized COM-Poisson regression
model.

This paper is organized as follows. In \autoref{background} we present
the COM-Poisson distribution and \cite{Huang2017} parametrization. The
proposed reparametrization and assessement of moments approximations is
considered in the \autoref{reparametrization}. In the
\autoref{estimation-and-inference} we show the estimation and inference
methods based on likelihood function whose properties are assess in
\autoref{simulation-study} by simulations. Applications of
reparametrized COM-Poisson regression model are illustrated in
\autoref{case-studies} with three case studies. The analysis are
performed using the software R \citep{Rcore2017}. The source code and
data sets are available in supplementary material\footnote{Available on
  \texttt{\url{http://www.leg.ufpr.br/~eduardojr/papercompanions}}
  \label{papercompanion}.}.

\section{Background}
\label{background}

The COM-Poisson distribution generalizes the Poisson distribution in
terms of the ratio between the probabilities of two consecutive values,
with addition of a precision parameter \cite{Sellers2010}. Let $Y$ be a
COM-Poisson random variable, then
$$\frac{\Pr(Y=y-1)}{\Pr(Y=y)} = \frac{y^\nu}{\lambda}$$ while for the
Poisson distribution this ratio is $\frac{y}{\lambda}$. This allows the
distribution deals with overdispersion or underdispersion cases. The probability mass
function for the COM-Poisson random variables takes the form
\begin{equation}
  \label{eqn:pmf-cmp}
  \Pr(Y=y \mid \lambda, \nu) = \frac{\lambda^y}{(y!)^\nu Z(\lambda, \nu)},
  \qquad y = 0, 1, 2, \ldots,
\end{equation}
where $\lambda > 0$, $\nu \geq 0$ and $Z(\lambda, \nu) =
\sum_{j=0}^\infty \frac{\lambda^j}{(j!)^\nu}$ is a normalizing
constant that depends on both parameters.

The $Z(\lambda, \nu)$ series diverges theoretically only when $\nu=0$
and $\lambda \geq 1$, but numerically for small values of $\nu$ combined
with large of $\lambda$, the sum is so huge it causes
\textit{overflow}. \autoref{tab:convergenceZ} shows the calculated
normalizing constants using a thousand increments, that is,
$\sum_{j=0}^{1000}\lambda^j/(j!)^\nu$ for different values of $\lambda$
and $\phi$ \footnote{For $\lambda=1$ e $\nu=0$ we show Inf, although the
  numerically value is $1000$, since the series clearly diverges.}.

<<convergenceZ, echo=FALSE, results="asis">>=

#-----------------------------------------------------------------------
# Convergence of Z(lambda, nu) constant
computeZ <- function(lambda, nu, maxit = 1e4, tol = 1e-5) {
    z <- vector("numeric", maxit)
    j = 1
    z[j] <- exp(j * log(lambda) - nu * lfactorial(j))
    while (abs(z[j] - 0) > tol && j <= maxit) {
        j = j + 1
        z[j] <- exp(j * log(lambda) - nu * lfactorial(j))
    }
    if (lambda == 1 & nu == 0) z <- Inf
    return(sum(z, na.rm = TRUE) + 1)
}

grid <- expand.grid(
    lambda = c(0.5, 1, 5, 10, 30, 50),
    nu = seq(0, 1, length.out = 11))
grid$z <- apply(grid, 1, function(par) {
    computeZ(lambda = par[1], nu = par[2], maxit = 1e4)
})

xt <- xtabs(z ~ nu + lambda, data = grid)
caption <- paste(
    "Values for $Z(\\lambda, \\nu)$ constant (calculated numerically)",
    "for values of $\\lambda$ (0.5 to 50) and $\\phi$ (0 to 1)")

# Remove excessive scientific notation
xt2 <- gsub("e\\+00", "    ", format(xt))
xt2 <- gsub("\\|([0-9].{2}) \\|", "\\|**\\1** \\|", xt2)
print(xtable(xt2, digits = -2,
             caption = caption,
             align = "C|CCCCCC",
             # align = "ccccccc",
             label = "tab:convergenceZ"),
      include.colnames = FALSE,
      tabular.environment = "tabularx",
      width = "\\textwidth",
      add.to.row = list(
          pos = list(0, 0),
          command = c(
              " \\multicolumn{7}{c}{$\\bm{\\lambda}$} \\\\\n",
              "$\\bm{\\nu}$ & 0.5 & 1 & 5 & 10 & 30 & 50 \\\\\n")
      ))

@

An undesirable feature of COM-Poisson distribution is that the moments
cannot be solved in closed form. \citet{Shmueli2005} and
\citet{Sellers2010} using an asymptotic approximation for
$Z(\lambda,\nu)$, showed that an approximate form can be
\begin{equation}
  \label{eqn:mean-approx}
  E(Y) \approx \lambda^{1/\nu} - \frac{\nu - 1}{2\nu} \qquad
  \textrm{and} \qquad
  \textrm{var}(Y) \approx \frac{\lambda^{1/\nu}}{\nu}\,,
\end{equation} which is particularly accurate for  $\nu \leq 1$ or
$\lambda > 10$. The authors also shows that the mean-variance
relationship can be approximate by $\frac{1}{\nu}E(Y)$. In
\autoref{reparametrization} we assess the accucary of these
approximations.

A regression formulation for COM-Poisson models was proposed by
\citet{Sellers2010}, using the original parametrization. In this case,
the COM-Poisson regression is $\eta(E(Y_i)) = \log(\lambda_i)$ and the
relationship between $E(Y_i)$ and $\bm{x}_i$ is modelled
indirectly. \citet{Huang2017} shows how the means of COM-Poisson
distributions can be modelled directly. In the \autoref{eqn:pmf-cmp},
\Citeauthor{Huang2017} proposes that the parameter $\lambda$ as a
function of $\mu$ and $\nu$, given by the solution to
\begin{equation*}
  \sum_{j=0}^{\infty} (j - \mu) \frac{\lambda^j}{(y!)^\nu} = 0\,.
\end{equation*}
Thus the mean-parametrized COM-Poisson regression is $\eta(E(Y_i)) =
\log(\mu_i)$. In this article, we presented an alternative
mean-parametrization of COM-Poisson models to avoid the limitations of
original parametrization and complexity of \Citeauthor{Huang2017}
parametrization.

\section{Reparametrization}
\label{reparametrization}

The proposed reparametrization of COM-Poisson models is based on
mean approximation (\autoref{eqn:mean-approx}). We introduced new
parameter $\mu$, using the mean approximation
\begin{equation}
  \label{eqn:repar-cmp}
  \mu = h(\lambda, \nu) = \lambda^{1/\nu} - \frac{\nu - 1}{2\nu}
  \quad \Rightarrow \quad
  \lambda = h^{-1}(\mu, \nu) = \left (\mu +
    \frac{(\nu - 1)}{2\nu} \right )^\nu.
\end{equation}
The precision parameter is taken on the log scale to avoid restrictions
on the parameter space, $\phi = \log(\nu)$, $\phi \in \mathbb{R}$. The
interpretation of $\phi$ are the same to $\nu$, but in other scale. For
$\phi<0$ we have the overdispersed case and $\phi>0$ underdispersed,
relatives to Poisson distribution. $\phi=0$ represents the special case
that the COM-Poisson recovers to Poisson.

<<data-approx, include=FALSE, cache=TRUE>>=

#-----------------------------------------------------------------------
# Study the approximation

#-------------------------------------------
# Mean and variance relationship
aux <- expand.grid(
    mu = seq(3, 30, length.out = 50),
    phi = seq(-1.5, 1.8, length.out = 50))

moments <- mapply(FUN = calc_moments,
                  mu = aux$mu,
                  phi = aux$phi,
                  MoreArgs = list(sumto = 300),
                  SIMPLIFY = FALSE)
grid <- cbind(aux, t(do.call(cbind, moments)))
grid <- transform(grid, va = mu / exp(phi))

@

In order to assess the accuracy of the moments approximations
(\autoref{eqn:mean-approx}) we presented the quadratic errors of moments
approximations in \autoref{fig:approx-plot}(a) and
\ref{fig:approx-plot}(b). Quadratic errors are obtained by
$[\mu - E(Y)]^2$ for expectation and by
$[ \mu \exp(-\phi) - \text{var}(Y)]^2$, where $E(Y)$ and var$(Y)$ are
numerically calculated using the moments definitio. The dotted lines
represent the border between the regions $\nu \leq 1$ and
$\lambda > 10^\nu$, in the $(\mu\,,\phi)$ scale.

<<approx-plot, fig.height=3, fig.width=8, fig.cap="Quadratic errors for expectation (a) and variance (b) approximation, dotted lines representing the restriction for good approximations by \\cite{Shmueli2005}. Mean-variance relationship (c), dotted line represents $E(Y)=\text{var}(Y)$.">>=

#-------------------------------------------
# Errors in approximations for E[Y] and V[Y]
grid <- transform(grid,
                  emu = (mu - mean)^2,
                  eva = (va - var)^2)

myreg <- colorRampPalette(c("gray90",  "gray20"))
xy1 <- levelplot(emu ~ phi + mu, data = grid,
                 aspect = "fill",
                 col.regions = myreg(100),
                 xlab = expression(phi),
                 ylab = expression(mu),
                 sub = "(a)",
                 colorkey = list(space = "top"),
                 par.settings = ps2,
                 panel = function(x, y, z, ...) {
                     panel.levelplot(x, y, z, ...)
                     panel.curve(10 - (exp(x) - 1)/(2 * exp(x)),
                                 lty = 2)
                     panel.abline(v = 0, lty = 2)
                 })

xy2 <- levelplot(eva ~ phi + mu, data = grid,
                 aspect = "fill",
                 col.regions = myreg(100),
                 xlab = expression(phi),
                 ylab = expression(mu),
                 sub = "(b)",
                 colorkey = list(space = "top"),
                 par.settings = ps2,
                 panel = function(x, y, z, ...) {
                     panel.levelplot(x, y, z, ...)
                     panel.curve(10 - (exp(x) - 1)/(2 * exp(x)),
                                 lty = 2)
                     panel.abline(v = 0, lty = 2)
                 })

#-------------------------------------------
# Mean-variance relantionship
xy3 <- xyplot(var ~ mean,
              groups = phi,
              data = grid,
              type = "l",
              lwd = 2,
              axis = axis.grid,
              xlab = expression(E(Y)),
              ylab = expression(var(Y)),
              sub = "(c)",
              legend = list(
                  top = list(
                      fun = draw.colorkey,
                      args = list(
                          key = list(
                              space = "top",
                              col = myreg(length(unique(grid$phi))),
                              at = unique(grid$phi),
                              draw = FALSE)))),
              par.settings = modifyList(
                  ps2, list(superpose.line = list(
                                col = myreg(length(unique(grid$phi))))
                            )
              ),
              panel = function(x, y, ...) {
                  panel.xyplot(x, y, ...)
                  panel.curve(1*x, min(x), max(x), lty = 2)
              },
              page = function(...) {
                  grid.text(expression(phi),
                            just = "bottom",
                            x = unit(0.15, "npc"),
                            y = unit(0.83, "npc"))
              })

print(xy1, split = c(1, 1, 3, 1), more = TRUE)
print(xy2, split = c(2, 1, 3, 1), more = TRUE)
print(xy3, split = c(3, 1, 3, 1), more = TRUE)

@

The results in \autoref{fig:approx-plot} show that the mean
approximation is accurate, the maximum of the quadratic errors is
\Sexpr{max(grid[["emu"]])} in the range evaluated. For the variance
approximation, the maximum is \Sexpr{max(grid[["eva"]])} and this occurs
to negative values of $\phi$. Interestingly, the errors are bigger to
negative values of $\phi$ and appear to have no relation with $\mu$
against the regions gives by \citet{Shmueli2005} ($\phi \leq 0$ and
$\mu > 10 - \frac{\exp(\phi) - 1}{2\exp(\phi)}$).
\autoref{fig:approx-plot}(c) shows that the relationship
between mean and variance is linear with slope controlled by precision
parameter $\phi$, but not in the form $\mu \exp(-\phi)$. In terms of
moments, this leads to a specification indistinguishable from the
quasi-Poisson.

The results presented in \autoref{fig:approx-plot}(a) support the
proposed reparametrization. Replacing $\lambda$ and $\nu$ as function of
$\mu$ and $\phi$ in \autoref{eqn:pmf-cmp}, the reparametrized
distribution takes the form
\begin{equation}
  \label{eqn:pmf-cmpmu}
  \Pr(Y=y \mid \mu, \phi) =
  \left ( \mu +\frac{ e^\phi-1}{2e^\phi} \right )^{ye^\phi}
  \frac{(y!)^{-e^\phi}}{Z(\mu, \phi)},
  \qquad y = 0, 1, 2, \ldots\,,
\end{equation} where $\mu > 0$. We denotes this distribution as
COM-Poisson$_\mu$. In \autoref{fig:pmf-cmp} we show the shapes of
COM-Poisson$_\mu$ distribution.

<<pmf-cmp, fig.height=3, fig.width=7, fig.cap="Shapes of the COM-Poisson distribution for different parameter values.">>=

#-------------------------------------------
# COM-Poisson probabilities
parg <- expand.grid(mu = c(2, 8, 15), phi = c(-0.7, 0, 0.7))
y <- 0:30
py <- mapply(FUN = dcmp,
             mu = parg$mu,
             phi = parg$phi,
             MoreArgs = list(y = y, sumto = 100),
             SIMPLIFY = FALSE)
parg <- cbind(parg[rep(1:nrow(parg), each = length(y)), ],
              y = y, py = unlist(py))

leg_phi <- parse(
    text = paste("phi == \"",
                 formatC(unique(parg$phi), 1, format = "f"),
                 "\""))
barchart(py ~ y | factor(mu),
         groups = factor(phi),
         data = parg,
         horizontal = FALSE,
         layout = c(NA, 1),
         as.table = TRUE,
         axis = axis.grid,
         origin = 0,
         xlim = extendrange(y, f = 0.01),
         border = "transparent",
         scales = list(x = list(at = pretty(y))),
         ylab = expression(P(Y==y)),
         xlab = expression(y),
         par.settings = list(
             superpose.polygon = list(
                 col = c("gray40", "gray60", "gray10")),
             superpose.line = list(
                 col = c("gray40", "gray60", "gray10"),
                 lwd = 2)
         ),
         auto.key = list(
             columns = 3,
             rectangles = FALSE,
             lines = TRUE,
             text = leg_phi
         ),
         strip = strip.custom(
             strip.names = TRUE,
             var.name = expression(mu == ""),
             sep = ""))

@

\section{Estimation and Inference}
\label{estimation-and-inference}

In this section we shall describe the estimation and inference methods
adopted in this paper. The two forms of COM-Poisson models are fitted by
maximization of the log-likelihood function. The log-likelihood function
based on a set of independent observation $y_i$, $i=1,2,\ldots,n$ for
COM-Poisson$_\mu$ has the following form,
\begin{equation}
  \label{eqn:ll-rcmp}
  \ell = \ell(\bm{\beta}, \phi \mid \bm{y}) =
  e^\phi \left [
    \sum_{i=1}^n y_i
    \log \left( \mu_i + \frac{e^\phi-1}{2e^\phi} \right ) -
    \sum_{i=1}^n \log(y_i!) \right ] -
  \sum_{i=1}^n \log(Z(\mu_i, \phi)),
\end{equation}
where $\mu_i = \exp(\bm{x}_i^\top\bm{\beta})$, with
$\bm{x}_i^\top = (x_{i1},\, x_{i2},\, \ldots,\, x_{ip})$ the vector of
known covariates for the $i$-th observation, and $(\bm{\beta},\, \phi)
\in \mathbb{R}^{p+1}$. The normalizing constant $Z(\mu_i, \phi)$ is
given by
\begin{equation*}
  Z(\mu_i, \phi) = \sum_{j=0}^\infty \left [ \left (
    \mu_i + \frac{e^\phi - 1}{2e^\phi} \right )^{je^\phi}
  \frac{1}{(j!)^{e^\phi}} \right ]
\end{equation*}
To compute the log-likelihood function we need the computation of an
infinite serie for each observation, which makes the computation
expensive for regions in the parameter space where the convergence of
sum $Z$ is slow.

Parameter estimation requires numerical maximization of
\autoref{eqn:ll-rcmp}. Since $\ell$ derivatives cannot be solved in
closed forms, we adopted the BFGS algorithm \cite{Nocedal1995} that uses
numerical estimate for the Hessian matrix
$\mathcal{H}(\bm{\theta})$. Standard errors for regression coefficients
are obtained based on the observed information matrix
$\mathcal{I}(\bm{\theta})$, where
$\mathcal{I}(\bm{\theta}) = -\mathcal{H}(\bm{\theta})$ numerically
calculated by BFGS algorithm. Confidence intervals for $\hat{\mu}_i$ are
obtained by using the delta method \citep[p. 89]{Pawitan2001}.

The parameter estimation for COM-Poisson model in the original
parametrization is analogous to presented for COM-Poisson$_\mu$, however
considering \autoref{eqn:ll-rcmp} in terms of $\lambda$. Even for the
standard COM-Poisson distribution, the precision parameter is taken on
the log scale, to avoid restriction of the parameter space.

Values of the maximized log-likelihood, the Akaike criterion (AIC) and
Bayesian criterion (BIC) are computed to compare the parametric
models. As a baseline model, in the applications we fitted the
quasi-Poisson model \citep{Wedderburn1974}. This approach is based on
second-moment assumption that allows more flexibility to model. In this
case the variance of the response variable is fixed by a additional
parameter $\sigma$, $\textrm{var}(Y_i)=\sigma \mu_i$. These models are
fitted in software R \citep{Rcore2017} by function \texttt{glm(...,
  family = quasipoisson)}.

\section{Simulation study}
\label{simulation-study}

In this section we conducted a simulation study to assess the properties
of the proposed model and estimators. We considered average counts
varying from 3 to 27 according to a regression model structure with a
continuous variable $\bm{x}_1$ between 0 and 1 and a categorical
variable $\bm{x}_2$ with three levels. Then the systematic component is
given by $\bm{\mu} = \exp(\beta_0 + \beta_1 \bm{x}_1 + \beta_{21}
\bm{x}_{21} + \beta_{22} \bm{x}_{22})$, where $\bm{x}_{21}$ and
$\bm{x}_{22}$ are \textit{dummy} variable for $\bm{x}_2$. With respect
to the precision parameter $\phi$, we fixed it to four values: $-1.6$,
$-1.0$, $0.0$ and $1.8$; in orderto have strong overdispersed, moderate
overdispersed, equidispersed and underdispersed counts
respectively. \autoref{fig:justpars} shows the variation of the average
counts (left) and variation of dispersion index (E$(Y)$/var$(Y)$) for
each fixed value of the precision parameter $\phi$. This configuration
allows us to evaluate the properties of the estimators in extreme
situations such as high counts and low dispersion, and low counts and
high dispersion.

In order to assess the consistency of the estimators we considered four
different sample sizes: 50, 100, 300 and 1000; generating 1000 datasets
in each case. In \autoref{fig:bias-plot} we show the bias of the
estimators for each scenario (combination between values of the
precision parameter and samples sizes) together with a confidence
interval calculated as average bias plus and minus $\Phi(0.975)$ times
the standard error. The scales are standardized for each parameter by
dividing the bias by the standard error obtained for the sample of size
50.

%-----------------------------------------------------------------------
% Explain the simulation
% \begin{algorithm}
% \caption{Steps in simulation study.}
% \label{alg:simulation}
%   \Begin{
%   $\bm{\beta} = \begin{bmatrix} \beta_0 & \beta_1 & \beta_{21} &
%     \beta_{22} \end{bmatrix}^\top = \begin{bmatrix} 2.0 &
%     0.5 & 0.8 & -0.8 \end{bmatrix}^\top$\;
%   \For{$n \in \{50, 100, 300, 1000\}$}{
%     set $\bm{x}_1$ as a sequence, with $n$ elements, between $0$ and
%     $1$\;
%     set $\bm{x}_2$ as a repetition, with $n$ elements, of three
%     categories\;
%     compute $\bm{\mu}$ using $\bm{\mu} = \exp(\beta_0 + \beta_1 \bm{x}_1
%     +  \beta_{21} \bm{x}_{21} + \beta_{22} \bm{x}_{22})$, where
%     $\bm{x}_{21}$ and $\bm{x}_{22}$ are \textit{dummy} variable for
%     $\bm{x}_2$\;
%     \For{$\phi \in \{-1.6, -1.0, 0.0, 1.8\}$}{
%       \Repeat{$1000$ times}{
%         simulate $\bm{y}$ from COM-Poisson distribution with $\bm{\mu}$
%         and $\phi$ parameters\;
%         fit COM-Poisson$_\mu$ regression model to $\bm{y}$ data with
%         $\bm{X} = \begin{bmatrix} \bm{1} & \bm{x}_1 & \bm{x}_{21} &
%           \bm{x}_{22} \end{bmatrix}$ design matrix\;
%         get $\hat{\bm{\theta}} = \begin{bmatrix} \hat{\phi} & \hat{\beta}_0 &
%           \hat{\beta}_1 & \hat{\beta}_{21} & \hat{\beta}_{22}
%         \end{bmatrix}^\top$\;
%         get confidence intervals for $\hat{\bm{\theta}}$ by quadratic
%         approximation at the maximum likelihood estimate
%         (assumes $\hat{\bm{\theta}} \sim \mathcal{N}(\hat{\bm{\theta}},
%         \sqrt{-\bm{v}})$, where $\bm{v}$ is a diagonal of the inverse of
%         the Hessian matrix)\;
%       }
%     }
%   }
% }
% \end{algorithm}

<<load-simulation, cache=TRUE>>=

# Configuration
B <- 1000
beta <- c("b0" = 2, "b1" = 0.5, "b21" = 0.8, "b22" = -0.8)
phis <- c(0, -1.6, -1, 1.8)
names(phis) <- sprintf("phi=%s", phis)

sizes <- c(50, 100, 300, 1000)
names(sizes) <- sprintf("n=%s", sizes)

# Load results
results <- readRDS("./codes/simulation.rds")

@

<<justpars, fig.height=4, fig.width=9, fig.cap="Average counts (left) and dispersion indexes (right) for each scenario considered in the simulation study.">>=

#-------------------------------------------
# Justify the choices of the parameters simulation
x1 <- seq(0, 1, length.out = 100)
x2 <- rep(letters[1:3], length.out = 100)
X <- model.matrix(~ x1 + x2)
mu <- exp(X %*% beta)

daexplain <- ldply(lapply(phis, function(phi) {
    va <- compute_variance(mu, phi, sumto = 300)
    data.frame(mu = mu, va = va, di = va / mu, x1 = x1, x2 = x2)
}), .id = "phi")

labx2 <- expression("Level of categorical variable"~(x[2]))
labx1 <- expression("Values of continous variable"~(x[1]))

fl <- parse(text = gsub("=", "==", levels(daexplain$phi)))
xy1 <- xyplot(mu ~ x1, groups = x2,
              type = c("g", "l"),
              lwd = 2,
              xlab = labx1,
              ylab = "Average count",
              auto.key = list(
                  column = 3,
                  points = FALSE,
                  lines = TRUE,
                  title = labx2,
                  cex.title = 1.1
              ))
xy2 <- xyplot(di ~ x1 | phi, groups = x2,
              type = c("g", "l"),
              lwd = 2,
              # scales = "free",
              layout = c(2, 2),
              xlab = labx1,
              ylab = "Dispersion index",
              data = daexplain,
              strip = strip.custom(factor.levels = fl))

print(xy1, split = c(1, 1, 2, 1), more = TRUE)
print(xy2, split = c(2, 1, 2, 1), more = FALSE)

@

<<bias-data, cache=TRUE>>=

#-----------------------------------------------------------------------
# Compute standardized bias
std <- lapply(results["n=50"], function(x) {
    ind <- names(x); names(ind) <- ind
    lapply(ind, function(y) {
        bhat <- t(vapply(x[[y]], "[[", double(5), "coef"))
        real <- matrix(c(phis[y], beta), byrow = TRUE,
                       nrow = B, ncol = 5)
        matrix(apply(bhat - real, 2, sd, na.rm = TRUE),
               byrow = TRUE, nrow = B, ncol = 5)
    })
})

aux <- ldply(lapply(results, function(x) {
    ind <- names(x); names(ind) <- ind
    out <- lapply(ind, function(y) {
        bhat <- t(vapply(x[[y]], "[[", double(5), "coef"))
        real <- matrix(c(phis[y], beta), byrow = TRUE,
                       nrow = B, ncol = 5)
        # (bhat - real)                 # raw bias
        (bhat - real) / std[[1]][[y]] # standardized bias
    })
    ldply(out, .id = "phi")
}), .id = "n")

# Organize the results
aux <- na.omit(aux)
bias <- gather(aux, param, bias, phi2:b22, factor_key = TRUE)
bias$phi <- ordered(bias$phi, c("phi=-1.6", "phi=-1", "phi=0",
                                "phi=1.8"))

@

The results in \autoref{fig:bias-plot} show that for all dispersed
levels, both the expected bias and standard error tend to 0 as the
sample size is increased. The estimators for the regression parameters
are unbiased, consistency and their empirical distributions are
symmetric. For the precision parameter, the estimator is asymptotically
unbiased, in small samples the parameter is overestimated and the
empirical distribution is slightly right-skewed.
\autoref{fig:coverage-plot} presents the confidence interval coverage
rate based on quadratic approximation at the maximum likelihood estimate
(assumes $\hat{\bm{\theta}} \sim \mathcal{N}(\hat{\bm{\theta}},
\sqrt{\bm{v}})$, where $\bm{v}$ is a diagonal of the inverse of the
observed information matrix) by sample size and dispersed levels.

<<bias-plot, cache=TRUE, fig.height=5, fig.width=8, fig.cap="Distributions of standardized bias (gray points) and average with confidence intervals (black segments) by differents sample sizes and dispersion levels.">>=

# Distributions of bias and average with confidence intervals
ci <- function(x) {
    ci <- mean(x) + c(-1, 0, 1) * qnorm(0.975) * sd(x)
    names(ci) <- c("lwr", "fit", "upr")
    ci
}
cidata <- aggregate(bias ~ n + phi + param, data = bias, ci)

key <- list(
    type = "o",
    divide = 1,
    columns = 4,
    title = "Sample size",
    cex.title = 1.1,
    lines = list(pch = c(21:24), cex = 0.8),
            text = list(names(sizes))
)

fl <- parse(text = gsub("=", "==", levels(bias$phi)))
yl <- parse(text = c("hat(phi)",
                     paste0("hat(beta)[", c(0, 1, 21, 22), "]")))

xyplot(param ~ bias | phi, groups = n, data = bias,
       panel = panel.superpose,
       layout = c(4, 1),
       key = key,
       strip = strip.custom(factor.levels = fl),
       scales = list(y = list(labels = yl)),
       ylab = "",
       xlab = "Standardized Bias",
       jitter.y = TRUE,
       factor = 0.1,
       gap = 0.3,
       alpha = 0.6,
       col = "gray80",
       cex = 0.7,
       panel.groups = function(x, y, group.number,
                               subscripts, gap, ...){
           noise <- centfac(factor(levels(bias$n)), space = gap)
           noise <- sort(noise)
           panel.xyplot(x, y + noise[group.number], ...)
       }) +
    as.layer(
        segplot(param ~ bias[, "lwr"] + bias[, "upr"] | phi,
                centers = bias[, "fit"],
                data = cidata,
                draw = FALSE,
                horizontal = TRUE,
                layout = c(4, 1),
                groups = n, gap = 0.3,
                key = key,
                pch = 21:24,
                cex = 0.7,
                lwd = 2,
                panel = function(...) {
                    panel.groups.segplot(...)
                    panel.abline(v = 0, lty = 2)
                    panel.abline(h = 1:5, col = "lightgray", lty = 2)
                })
    )
@

<<coverage-data>>=

#-----------------------------------------------------------------------
# Compute coverage rate
aux <- ldply(lapply(results, function(x) {
    ind <- names(x); names(ind) <- ind
    out <- lapply(ind, function(y) {
        ind <- lapply(x[[y]], function(z) {
            real <- c(phis[[y]], beta)
            cint <- z[["cint"]]
            ind <- as.integer(cint[, 1] < real & cint[, 2] > real)
            names(ind) <- c("phi2", names(beta))
            ind
        })
        do.call("rbind", ind)
    })
    ldply(out, .id = "phi")
}), .id = "n")

# Organize the results
aux <- na.omit(aux)
coverage <- gather(aux, param, coverage, phi2:b22, factor_key = TRUE)
coverage$phi <- ordered(coverage$phi, c("phi=-1.6", "phi=-1", "phi=0",
                                        "phi=1.8"))
coverage$n <- as.numeric(gsub("n=([0-9]+)", "\\1", coverage$n))
covdata <- aggregate(coverage ~ n + phi + param, data = coverage,
                     function(x) sum(x == 1) / length(x))

@

The coverage rates in \autoref{fig:coverage-plot} show that for the
regression parameters the empirical coverage rates are close to the
nominal level of 95\% for sample sizes greater than 100 and all
dispersed levels. For the precision parameter the empirical coverage
rates are slightly lower than the nominal level, however, they become
closer for large samples. The worst scenario is when we have small
sample size and strong overdispersed counts.

<<coverage-plot, fig.height=4, fig.width=9, fig.cap="Coverage rate based on confidence intervals obtained by quadratic approximation for differents sample sizes and dispersion levels.">>=

fl <- parse(text = gsub("=", "==", levels(bias$phi)))
yl <- parse(text = c("hat(phi)",
                     paste0("hat(beta)[", c(0, 1, 21, 22), "]")))
xyplot(coverage ~ n | param,
       type = c("g", "p", "l"),
       groups = phi,
       data = covdata,
       pch = 19,
       lwd = 1.5,
       layout = c(NA, 1),
       xlab = "Sample size",
       ylab = "Coverage rate",
       par.settings = list(
           layout.heights = list(strip = 1.5)
       ),
       key = list(
           column = 4,
           type = "o",
           divide = 1,
           text = list(fl),
           lines = list(pch = 19, col = cols[1:4])
       ),
       strip = strip.custom(
           factor.levels = yl
       ),
       panel = function(x, y, ...) {
           panel.xyplot(x, y, ...)
           panel.abline(h = 0.95, lty = 2)
       })

@

\section{Case studies}
\label{case-studies}

In this section, we report three illustrative examples of real count
data analysis obtained from experimental research. We considered as
alternative models for the analysis the standard Poisson model, the
COM-Poisson model in two forms (original and mean parametrization) and
Quasi-Poisson model, based on second-moment assumptions. The data sets
and \texttt{R} codes for their analysis are available on on-line
supplements.

\subsection{Artificial defoliation in cotton phenology}
\label{case-cotton}

This example relates to cotton plants (\textit{Gossypium hirsutum})
submitted to five levels of artificial defoliation (\texttt{des}) and
crossed with five growth stages (\texttt{est}). The purpose of this
research was to study the effect of defoliation levels at different
growth stages of cotton plants on the cotton production, expressed by
the number of bools produced. The study was conducted in a greenhouse
and the experimental design was completely randomized with five
replicates. This data set was analysed by \citet{Zeviani2014} using the
\textit{Gamma-Count} distribution.

Following the results of \citet{Zeviani2014}, we considered the linear
predictor specified by
$$\log(\mu_{ij}) = \beta_0 + \beta_{1j} \textrm{def}_i + \beta_{2j}
\textrm{def}_i^2$$
where $\mu_{ij}$ is the expected number of cotton bolls for the $i$-th
defoliation level ($i=$ 1: 0\%, 2: 25\%, 3: 50\%, 4: 75\% e 5: 100\%)
and $j$-th growth stage ($j$ = 1: vegetative, 2: flower bud, 3: blossom,
4: boll, 5: boll open), that is, we have a second order effect of
defoliation in each growth stage. The parameters estimates and
goodness-of-fit measures for the Poisson, COM-Poisson, COM-Poisson$_\mu$
and quasi-Poisson models are presented in \autoref{tab:coef-cotton}.

<<fit-cotton, cache=TRUE, include=FALSE>>=

#-----------------------------------------------------------------------
# Load data
# data(cottonBolls, package = "cmpreg")
cottonBolls <- read.table("./data/cottonBolls.txt",
                          header = TRUE, sep = "\t")
cottonBolls$est <- ordered(
    cottonBolls$est,
    c("vegetative", "flower bud", "blossom", "boll", "boll open")
)

#-----------------------------------------------------------------------
# Fit models
mnames <- c("PO", "C1", "C2", "QP")

# Predictor, following Zeviani et al. (2014)
form1 <- ncap ~ est:(des + I(des^2))

m1PO <- glm(form1, data = cottonBolls, family = poisson)
time11 <- system.time(
    m1C1 <- fitcm(form1, data = cottonBolls, model = "CP", sumto = 50)
)
time12 <- system.time(
    m1C2 <- fitcm(form1, data = cottonBolls, model = "CP2", sumto = 50)
)
m1QP <- glm(form1, data = cottonBolls, family = quasipoisson)

models.ncap <- list(m1PO, m1C1, m1C2, m1QP)
names(models.ncap) <- mnames

# Numbers of calls to loglik and numerical gradient
c11 <- models.ncap$C1@details$counts
c12 <- models.ncap$C2@details$counts

# LRT between Poisson and COM-Poisson (test: phi == 0)
lrt.ncap <- getAnova(m1PO, m1C2)

@

\begin{table}[h]
\centering
\caption{Parameter estimates (Est) and ratio between estimate and
  standard error (SE) for the four model strategies for the cotton
  experiment.}
\label{tab:coef-cotton}
\begin{tabularx}{\textwidth}{lCCCCCCCC}
  \toprule
  & \multicolumn{2}{c}{Poisson} &
    \multicolumn{2}{c}{COM-Poisson} &
    \multicolumn{2}{c}{COM-Poisson$_\mu$} &
    \multicolumn{2}{c}{Quasi-Poisson} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
<<results-cotton, results="asis">>=

#-----------------------------------------------------------------------
# Goodness of fit measures and estimate parameters

# GoF measures
measures.ncap <- lapply(models.ncap, function(x)
    c("LogLik" = logLik(x), "AIC" = AIC(x), "BIC" = BIC(x)))

# Get the estimates
co1 <- coef(m1C2)
est <- lapply(models.ncap, FUN = function(x) getCoefs(x))
est.ncap <- do.call(cbind, est)

# Organize in table
pnames <- c("\\phi\\,,\\,\\sigma", "\\beta_0",
            paste0("\\beta_{1", 1:5, "}"),
            paste0("\\beta_{2", 1:5, "}"))
rownames(est.ncap) <- paste0("$", pnames, "$")
meds <- apply(do.call(cbind, measures.ncap), 1, function(x) {
    x <- formatC(x, getOption("digits"), format = "f")
    x <- gsub("NA", "---", x)
    paste(paste0("\\multicolumn{2}{c}{", x, "}"),
          collapse = " & ")
})
text_gof <- paste(paste(names(meds), "&", meds),
                  collapse = "\\\\\n ")

append_gof <- list(
    pos = list(nrow(est.ncap)),
    command = paste("\\specialrule{0.01em}{0.3em}{0.3em} \n",
                    text_gof, "\\\\\n",
                    "\\bottomrule"))
print.xtable(xtable(est.ncap, digits = 4,
                    label = "tab:coef-cotton"),
             hline.after = 0,
             only.contents = TRUE,
             add.to.row = append_gof)

@
\end{tabularx}
\end{table}

The results presented in \autoref{tab:coef-cotton} show that the
goodness-of-fit measures (maximum log-likelihood, AIC and BIC) are quite
similar between COM-Poisson and COM-Poisson$_\mu$. This shows that the
reparametrization does not change the model fit, as expected. The Poisson
model is clearly unsuitable, being overly conservative. The difference
between the log-likelihoods of the Poisson and COM-Poisson$_\mu$ models
is \Sexpr{lrt.ncap[2, 4]}. Comparing this value with the chi-square
distribution, with one degree of freedom, we have these models are
significantly different ($\phi \neq 0$). The estimated value of
precision parameter, $\hat{\phi}$ is \Sexpr{co1[1]}, indicating
underdispersion.

\autoref{tab:coef-cotton} also shows the advantage of COM-Poisson$_\mu$
(reparametrized), since the estimates are quite similar those obtained by
the Poisson model, whereas estimates obtained by the COM-Poisson model
in original parametrization are on a non-interpretable scale. The ratios
between estimates and their respective standard errors for the
COM-Poisson models are very close to ratios obtained by quasi-Poisson
model. However, it is important to note that COM-Poisson model is a
full parametric approach, that is, there is a probability distribution
associate to counts.

In the \autoref{fig:pred-cotton} are presented the observed values and
curves of fitted values with confidence intervals (95\%) as functions of
the defoliation level for each growth stage. The fitted values are same
for both models, but confidence intervals are bigger for Poisson model
because the equidispersion assumption. The results from COM-Poisson
models are consistent with those from the Gamma-Count model, fitted by
\cite{Zeviani2014}, Poisson-Tweedie model, fitted by \cite{Bonat2017}
and alternative COM-Poisson parametrization, fitted by \cite{Huang2017},
in that both methods indicate underdispersion and significant effects of
defoliation for the vegetative, blossom and boll growth stages.

<<pred-cotton, fig.height=3.5, fig.width=8.5, fig.cap="Scatterplots of the observed data and curves of fitted values with 95\\% confidence intervals as functions of the defoliation level for each growth stage.">>=

#-----------------------------------------------------------------------
# Prediction

# Data for prediction
pred <- with(cottonBolls,
             expand.grid(
                 est = levels(est),
                 des = seq(min(des), max(des), l = 20)
             ))
qn <- qnorm(0.975) * c(fit = 0, lwr = -1, upr = 1)

# Design matrix for prediction
X <- model.matrix(update(form1, NULL~.), pred)

# Considering Poisson
aux <- exp(confint(
    glht(m1PO, linfct = X), calpha = univariate_calpha())$confint)
colnames(aux) <- c("fit", "lwr", "upr")
aux <- data.frame(modelo = "Poisson", aux)
predPO.ncap <- cbind(pred, aux)

# Considering COM-Poisson
aux <- predictcm(m1C1, newdata = X)
aux <- data.frame(modelo = "COM-Poisson", aux)
predC1.ncap <- cbind(pred, aux)

# Considering COM-Poisson (mean parametrization)
aux <- predictcm(m1C2, newdata = X)
aux <- data.frame(modelo = "COM-Poisson2", aux)
predC2.ncap <- cbind(pred, aux)

# Considering Quasi-Poisson
aux <- exp(confint(
    glht(m1QP, linfct = X), calpha = univariate_calpha())$confint)
colnames(aux) <- c("fit", "lwr", "upr")
aux <- data.frame(modelo = "Quasi-Poisson", aux)
predQP.ncap <- cbind(pred, aux)

# Representing the confidence intervals
pred.ncap <- rbind(predPO.ncap, predC1.ncap, predC2.ncap, predQP.ncap)

# Legend
key <- list(columns = 4,
            cex = 0.9,
            lines = list(col = 1, lty = rev(1:4)),
            text = list(parse(
                text = c("'Poisson'", "'COM-Poisson'",
                         "'COM-Poisson'[mu]", "'Quasi-Poisson'"))
                ))

# Graph
xyplot(ncap ~ des | est,
       data = cottonBolls,
       layout = c(NA, 1),
       as.table = TRUE,
       grid = TRUE,
       type = "p",
       xlab = "Artificial defoliation level",
       ylab = "Number of bolls produced",
       spread = 0.05,
       key = key,
       alpha = 0.6,
       panel = panel.beeswarm) +
    as.layer(
        xyplot(fit ~ des | est,
               auto.key = TRUE,
               data = pred.ncap,
               groups = modelo,
               type = "l",
               layout = c(NA, 1),
               as.table = TRUE,
               col = 1,
               ly = pred.ncap$lwr,
               uy = pred.ncap$ upr,
               cty = "bands",
               fill = "gray80",
               alpha = 0.1,
               panel = panel.superpose,
               panel.groups = panel.cbH,
               prepanel = prepanel.cbH,
               lty = rev(1:4))
    )

@

In order to assess the orthogonality between $\bm{\mu}$ and $\phi$ in
the COM-Poisson$_\mu$ parametrization, \autoref{tab:corr-cotton}
presents the empirical correlations between precision and regression
parameters, obtained from the estimated variance and covariance
matrix. The correlations are practically null considering the
COM-Poisson$_\mu$, whereas to the original parametrization they are
quite high. This reflects a better performance of the maximization
algorithm in mean-parametrization model, in this case COM-Poisson$_\mu$
fit was \Sexpr{time12[3] * 100 / time11[3]}\% slower then COM-Poisson
model. It is important to note that the initial values for the BFGS
algorithm are provided by Poisson fitted model, then in the COM-Poisson
mean-parametrized model the initial values are practically the maximum
likelihood estimates and the effort of maximization is on the precision
parameter $\phi$.

<<corr-cotton, results="asis">>=

#-----------------------------------------------------------------------
# Correlation between estimates
corr.ncap <- do.call("rbind",
                     lapply(models.ncap[c("C1", "C2")],
                            function(x) cov2cor(vcov(x))[1, -1]))

# Organize on table
rownames(corr.ncap) <- paste0("COM-Poisson", c("", "$_\\mu$"))
colnames(corr.ncap) <- gsub("beta", "hat{\\\\beta}", pnames[-1])

caption <- paste("Empirical correlations between $\\hat{\\phi}$ and",
                 "$\\hat{\\bm{\\beta}}$ for the two parametrizations",
                 "of COM-Poisson model fit to underdispersed data.")
print(xtable(corr.ncap,
             align = c("lccccccccccc"),
             caption = caption,
             digits = 3,
             label = "tab:corr-cotton"),
      size = "small",
      sanitize.rownames.function = identity,
      sanitize.colnames.function = function(x) sprintf("$%s$", x))

@

\subsection{Soil moisture and potassium doses on soy bean culture}
\label{case-soybean}

The second example is an experiment in a completely randomized block
with treatments in a $5\times 3$ factorial arrangement about soybean
(\emph{Glicine Max}). The aim of this study was to evaluate the effects
of potassium doses (\texttt{K}) applied to soil (0, 0.3, 0.6, 1.2 and
1.8 $\times$ 100mg dm$^{-3}$) and soil moisture (\texttt{umid}) levels
(37.5, 50, 62.5\%) on the soybean production. The experiment was
carried out in a greenhouse, in pots with two plants, and the count
variable measured was the number of grains per pot
\citep{Serafim2012}. \autoref{fig:desc-soy} (left) shows the number of
grains recorded for each combination of potassium dose and moisture
level, it is important to note the indication of a quadratic level of
the potassium levels. Most points in the sample variance vs. sample
means dispersion diagram (right) are above the identity line, suggesting
overdispersion (block effect not yet removed).

<<fit-soy, cache=TRUE, include=FALSE>>=

#-----------------------------------------------------------------------
# Load data
# data(soyaBeans, package = "cmpreg")
soyBeans <- read.table("./data/soyaBeans.txt",
                        header = TRUE, sep = "\t")
soyBeans$umid <- as.factor(soyBeans$umid)
soyBeans <- soyBeans[-74, ] # Incorrect observation
soyBeans <- transform(soyBeans, K = K / 100)

#-----------------------------------------------------------------------
# Fit models

# Predictor
form2 <-  ngra ~ bloc + umid * K + I(K^2)

m2PO <- glm(form2, data = soyBeans, family = poisson)
time21 <- system.time(
    m2C1 <- fitcm(form2, data = soyBeans, model = "CP", sumto = 700)
)
time22 <- system.time(
    m2C2 <- fitcm(form2, data = soyBeans, model = "CP2", sumto = 700)
)
m2QP <- glm(form2, data = soyBeans, family = quasipoisson)

models.ngra <- list(m2PO, m2C1, m2C2, m2QP)
names(models.ngra) <- mnames

# Numbers of calls to loglik and numerical gradient
c21 <- models.ngra$C1@details$counts
c22 <- models.ngra$C2@details$counts

# # Profile extra parameter
# profs.ngra <- lapply(list(c(m2C1, "phi"), c(m2C2, "phi2")),
#                      function(x) myprofile(x[[1]], x[[2]]))
# profs.ngra <- do.call("rbind", profs.ngra)

# LRT between Poisson and COM-Poisson (test: phi == 0)
lrt.ngra <- getAnova(m2PO, m2C2)

@

<<desc-soy, fig.pos="H", fig.height=4, fig.width=8, fig.cap="Number of grains per pot for each potassium dose and moisture level (left) and sample mean agains sample variance of the five replicates for each experimental treatment (right).">>=

#-----------------------------------------------------------------------
# Exploratory analysis

# Scatter plot
xy1 <- xyplot(ngra ~ K | umid,
              data = soyBeans,
              xlab = "Potassium fertilization level",
              ylab = "Number of grains per pot",
              type = c("p", "g", "smooth"),
              as.table =  TRUE,
              layout = c(2, 2),
              strip = strip.custom(
                  strip.names = TRUE, var.name = "moisture",
                  factor.levels = paste0(levels(soyBeans$umid), "%")))

# Sample variance vs sample mean (evidence in favor of the
# overdispersion).
mv <- soyBeans %>%
    group_by(K, umid) %>%
    summarise(mu = mean(ngra), va = var(ngra))
xlim <- ylim <- extendrange(c(mv$mu, mv$va), f = 0.05)

xy2 <- xyplot(va ~ mu,
              data = mv,
              type = c("p", "r", "g"),
              xlim = xlim,
              ylim = ylim,
              xlab = expression("Sample"~"mean"~(bar(y))),
              ylab = expression("Sample"~"variance"~(s^2)),
              panel = function(...) {
                  panel.xyplot(...)
                  panel.abline(a = 0, b = 1, lty = 2)
              })

print(xy1, split = c(1, 1, 2, 1), more = TRUE)
print(xy2, split = c(2, 1, 2, 1), more = FALSE)

@

For the analysis of this data set we proposed, based on descriptive
analysis \autoref{fig:desc-soy}, the linear predictor specified as a
linear doses for each humidity level and a global quadratic dose effect,
i.e
$$
\log(\mu_{ijk}) = \beta_0 + \gamma_i + \tau_j +
  \beta_{1}\texttt{K}_k + \beta_{2}\texttt{K}_k^2 +
  \beta_{3j}\texttt{K}_k
$$
with $i=$1: block II, 2: block III, 3: block IV e 4: block V; $j=$1:
50\% e 2: 62.5\%; and $k=$1: 0.0, 2: 0.3, 3: 0.6, 4: 1.2, 5: 1.8 100mg
dm$^{-3}$, where $\gamma_i$ is the effect of $i$-th block, $\tau_j$ is
the effect of $j$-th moisture level and $\beta_{3j}$ is the first order
potassium effect (\texttt{K}) for the $j$-th moisture level
(\texttt{umid}). \autoref{tab:coef-soy} presents the estimates, ratio
between estimate and standard error and goodness-of-fit measures for
the alternative models fitted.

The results in \autoref{tab:coef-soy} has similar interpretations to
results presented in \autoref{case-cotton}. The two parametrization of
COM-Poisson presented very similar goodness-of-fit measures and better
than Poisson model. The difference between the log-likelihoods of the
Poisson and COM-Poisson models is \Sexpr{lrt.ngra[2, 4]}, indicating
that $\phi$ is significantly different of zero. With respect to the
regression parameters, the similarities between models are analogous to
the previous section. Both models indicate effects of block, potassium
dose and moisture level, however the Poisson model indicates the effects
with greater significance, because it does not fit the extra
variability.

\begin{table}[ht]
\centering
\caption{Parameter estimates (Est) and ratio between estimate and
  standard error (SE) for the four model strategies for the soybean
  experiment.}
\label{tab:coef-soy}
\begin{tabularx}{\textwidth}{lCCCCCCCC}
  \toprule
  & \multicolumn{2}{c}{Poisson} &
    \multicolumn{2}{c}{COM-Poisson} &
    \multicolumn{2}{c}{COM-Poisson$_\mu$} &
    \multicolumn{2}{c}{Quasi-Poisson} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
<<results-soya, results="asis">>=

#-----------------------------------------------------------------------
# Goodness of fit measures and estimate parameters

# GoF measures
measures.ngra <- lapply(models.ngra, function(x)
    c("LogLik" = logLik(x), "AIC" = AIC(x), "BIC" = BIC(x)))

# Get the estimates
co2 <- coef(m2C2)
est <- lapply(models.ngra, FUN = function(x) getCoefs(x))
est.ngra <- do.call(cbind, est)

# Organize in table
pnames <- c("\\phi\\,,\\,\\sigma", "\\beta_0",
            paste0("\\gamma_{", 1:4, "}"),
            paste0("\\tau_{", 1:2, "}"),
            "\\beta_1", "\\beta_2",
            paste0("\\beta_{3", 1:2, "}"))

rownames(est.ngra) <- paste0("$", pnames, "$")
meds <- apply(do.call(cbind, measures.ngra), 1, function(x) {
    x <- formatC(x, getOption("digits"), format = "f")
    x <- gsub("NA", "---", x)
    paste(paste0("\\multicolumn{2}{c}{", x, "}"),
          collapse = " & ")
})
text_gof <- paste(paste(names(meds), "&", meds),
                  collapse = "\\\\\n ")

append_gof <- list(
    pos = list(nrow(est.ngra)),
    command = paste("\\specialrule{0.01em}{0.3em}{0.3em} \n",
                    text_gof, "\\\\\n",
                    "\\bottomrule"))
print.xtable(xtable(est.ngra, digits = 4,
                    label = "tab:coef-soy"),
             hline.after = 0,
             only.contents = TRUE,
             add.to.row = append_gof)

@
\end{tabularx}
\end{table}

The infinite sum $Z(\mu, \phi)$ in cases of oversidepersed counts needs
a greater upper bound for the convergence, making the computation of the
likelihood more expansive. In this case, the upper bound was 700. The
number of likelihood computations, on the BFGS algorithm, was
\Sexpr{c21[1]} and \Sexpr{c22[2]} for the original and
mean-parametrization and the time to fit was \Sexpr{time21[3]} and
\Sexpr{time22[3]} seconds respectively. This may be due the
well-behaviour of likelihood function for orthogonal parameters also,
best start values from Poisson model. In \autoref{tab:corr-soy} we
present the empirical correlations between regression and precision
parameter. The correlations are close to zero for the COM-Poisson$_\mu$
parametrization, indicating the empirical orthogonality between $\mu$
and $\phi$ on COM-Poisson$_\mu$ distribution.

<<corr-soy, results="asis">>=

##----------------------------------------------------------------------
## Correlation between estimates
corr.ngra <- do.call("rbind",
                     lapply(models.ngra[c("C1", "C2")],
                            function(x) cov2cor(vcov(x))[1, -1]))

## Organize on table
rownames(corr.ngra) <- paste0("COM-Poisson", c("", "$_\\mu$"))
colnames(corr.ngra) <- gsub("(beta|tau|gamma)", "hat{\\\\\\1}",
                            pnames[-1])

caption <- paste("Empirical correlations between $\\hat{\\phi}$ and",
                 "$\\hat{\\bm{\\beta}}$ for the two parametrizations",
                 "of COM-Poisson model fit to overdispersed data.")
print(xtable(corr.ngra,
             align = c("lccccccccccc"),
             caption = caption,
             digits = 3,
             label = "tab:corr-soy"),
      size = "small",
      sanitize.rownames.function = identity,
      sanitize.colnames.function = function(x) sprintf("$%s$", x))


@

The observed counts and fitted curves for each humidity level with
confidence bands are show in \autoref{fig:pred-soy}. The fitted values
are identical for both models, leading to the same conclusions. On the
other hand, confidence bands for the Poisson model are smaller due to
equidispersion assumption. Considering COM-Poisson models and
quasi-Poisson approach, the confidence bands are quite similar, which
shows their flexibility.

<<pred-soy, fig.pos="H", fig.height=3.8, fig.width=8, fig.cap="Dispersion diagrams of grain counts as function of potassium doses and humidity levels with fitted curves and confidence intervals (95\\%).">>=

#-----------------------------------------------------------------------
# Prediction

# Data for prediction
pred <- with(soyBeans,
             expand.grid(
                 bloc = factor(levels(bloc)[1], levels = levels(bloc)),
                 umid = levels(umid),
                 K = seq(min(K), max(K), l = 20)
             ))
qn <- qnorm(0.975) * c(fit = 0, lwr = -1, upr = 1)

# Design matrix for prediction
X <- model.matrix(update(form2, NULL ~ .), pred)
bl <- attr(X, "assign") == 1
X[, bl] <- X[, bl] + 1/(sum(bl) + 1)

# Considering Poisson
aux <- exp(confint(
    glht(m2PO, linfct = X), calpha = univariate_calpha())$confint)
colnames(aux) <- c("fit", "lwr", "upr")
aux <- data.frame(modelo = "Poisson", aux)
predPO.ngra <- cbind(pred, aux)

# Considering COM-Poisson
aux <- predictcm(m2C1, newdata = X)
aux <- data.frame(modelo = "COM-Poisson", aux)
predC1.ngra <- cbind(pred, aux)

# Considering COM-Poisson (mean parametrization)
aux <- predictcm(m2C2, newdata = X)
aux <- data.frame(modelo = "COM-Poisson2", aux)
predC2.ngra <- cbind(pred, aux)

# Considering Quasi-Poisson
aux <- exp(confint(
    glht(m2QP, linfct = X), calpha = univariate_calpha())$confint)
colnames(aux) <- c("fit", "lwr", "upr")
aux <- data.frame(modelo = "Quasi-Poisson", aux)
predQP.ngra <- cbind(pred, aux)

# Representing the confidence intervals
pred.ngra <- rbind(predPO.ngra, predC1.ngra, predC2.ngra, predQP.ngra)

# Legend
key <- list(columns = 4,
            cex = 0.9,
            lines = list(col = 1, lty = rev(1:4)),
            text = list(parse(
                text = c("'Poisson'", "'COM-Poisson'",
                         "'COM-Poisson'[mu]", "'Quasi-Poisson'"))
                )
            )

# Graph
update(xy1, layout = c(NA, 1), type = c("p", "g"),
       alpha = 0.6, key = key) +
    as.layer(
        xyplot(fit ~ K | umid,
               data = pred.ngra,
               groups = modelo,
               type = "l",
               col = 1,
               ly = pred.ngra$lwr,
               uy = pred.ngra$upr,
               cty = "bands",
               fill = "gray80",
               alpha = 0.1,
               panel = panel.superpose,
               panel.groups = panel.cbH,
               prepanel = cmpreg::prepanel.cbH,
               lty = rev(1:4))
    )

@


\subsection{Assessing toxicity of pollutants in aquatic systems}

The data set comes from an experiment to measure the reproductive
toxicity of a herbicide, nitrofen, on a species of zooplankton
(\textit{Ceriodaphnia dubia}). 50 animals were randomized into batches
of 10 and each batch was put in a solution with a measured concentration
of nitrofen (0, 0.8, 1.6, 2.35 e 3.10 mug$/10^2$litre)
(\texttt{dose}). Then the number of total live offspring was recorded
\cite{Bailer1994}.

<<fit-ovos, cahe=TRUE, include=FALSE>>=

#-----------------------------------------------------------------------
# Load data
# data(nitrofen, package = "boot")
# data(Paula, package = "labestData")
# nitrofen <- PaulaEx4.6.20
nitrofen <- read.table("./data/nitrofen.txt",
                       header = TRUE, sep = "\t")
nitrofen <- transform(nitrofen, dose = dose / 100)

#-----------------------------------------------------------------------
# Fit models
mnames <- c("PO", "C1", "C2", "QP")

# Predictors
form31 <-  novos ~ dose
form32 <-  novos ~ dose + I(dose^2)
form33 <-  novos ~ dose + I(dose^2) + I(dose^3)

predictors <- list("pred1" = form31, "pred2" = form32, "pred3" = form33)
fmodels.ovos <- lapply(predictors, function(form) {
    PO <- glm(form, data = nitrofen, family = poisson)
    C1 <- fitcm(form, data = nitrofen, model = "CP", sumto = 100)
    C2 <- fitcm(form, data = nitrofen, model = "CP2", sumto = 100)
    QP <- glm(form, data = nitrofen, family = quasipoisson)
    list("PO" = PO, "C1" = C1, "C2" = C2, "QP" = QP)
})

#-----------------------------------------------------------------------
# LRT for nested models

# Poisson
auxPO <- lapply(fmodels.ovos, function(x) x$PO)
do.call("getAnova", auxPO)

# COM-Poisson standard
auxC1 <- lapply(fmodels.ovos, function(x) x$C1)
do.call("getAnova", auxC1)

# COM-Poisson mean-parameterized
auxC2 <- lapply(fmodels.ovos, function(x) x$C2)
do.call("getAnova", auxC2)

# Quasi-Poisson
auxQP <- lapply(fmodels.ovos, function(x) x$QP)
do.call("getAnova", auxQP)

#--------------------------------------------
# Separe the choose models
form3 <- form33
m3PO <- fmodels.ovos$pred3$PO
m3C1 <- fmodels.ovos$pred3$C1
m3C2 <- fmodels.ovos$pred3$C2
m3QP <- fmodels.ovos$pred3$QP

models.ovos <- list(m3PO, m3C1, m3C2, m3QP)
names(models.ovos) <- mnames

# Numbers of calls to loglik and numerical gradient
models.ovos$C1@details$counts
models.ovos$C2@details$counts

@

For this data set we proposed three linear predictors. We compare the
performance of models using the likelihood ratio tests to select the
systematic part of model. The linear predictors are
\begin{center}
\begin{minipage}{12cm}
Predictor 1: $\log(\mu_i) = \beta_0 + \beta_1 \texttt{dose}_i$\\
Predictor 2: $\log(\mu_i) = \beta_0 + \beta_1 \texttt{dose}_i +
             \beta_2 \texttt{dose}_i^2$\\
Predictor 3: $\log(\mu_i) = \beta_0 + \beta_1 \texttt{dose}_i +
             \beta_2 \texttt{dose}_i^2 + \beta_3 \texttt{dose}_i^3$.
\end{minipage}
\end{center}

\begin{table}[ht]
\centering
\caption{Model fit measures and comparisons between predictors and models.}
\label{tab:anova-ovos}
\begin{tabularx}{\textwidth}{lCCCCCrC}
  \toprule
 Poisson & np & $\ell$ & AIC & 2(diff $\ell$) & diff np & P($>\rchi^2$) & \\
 \midrule
<<anova-ovos1, results="asis">>=

auxPO <- lapply(fmodels.ovos, function(x) x$PO)
tab <- do.call("getAnova", c(print = FALSE, auxPO))
tab <- cbind(tab, NA)
rownames(tab) <- paste("Preditor", rownames(tab))
digits <- c(1, 0, 3, 3, 3, 0, -2, 3)
print(xtable(tab, digits = digits),
      include.colnames = FALSE,
      hline.after = NULL,
      only.contents = TRUE)

@
\specialrule{0em}{0.5em}{0em} %% Apenas para espaçamento
  COM-Poisson & np & $\ell$ & AIC & 2(diff $\ell$) & diff np &
  P($>\rchi^2$) & $\hat{\phi}$ \\
  \midrule
<<anova-ovos2, results="asis">>=

auxC1 <- lapply(fmodels.ovos, function(x) x$C1)
tab <- do.call("getAnova", c(print = FALSE, auxC1))
tab <- cbind(tab, sapply(auxC1, function(x) coef(x)[1]))
rownames(tab) <- paste("Preditor", rownames(tab))
print(xtable(tab, digits = digits),
      include.colnames = FALSE,
      hline.after = NULL,
      only.contents = TRUE)

@
\specialrule{0em}{0.5em}{0em} %% Apenas para espaçamento
  COM-Poisson$_\mu$ & np & $\ell$ & AIC & 2(diff $\ell$) & diff np &
  P($>\rchi^2$) & $\hat{\phi}$ \\
  \midrule
<<anova-ovos3, results="asis">>=

auxC2 <- lapply(fmodels.ovos, function(x) x$C2)
tab <- do.call("getAnova", c(print = FALSE, auxC2))
tab <- cbind(tab, sapply(auxC2, function(x) coef(x)[1]))
rownames(tab) <- paste("Preditor", rownames(tab))
print(xtable(tab, digits = digits),
      include.colnames = FALSE,
      hline.after = NULL,
      only.contents = TRUE)

@
\specialrule{0em}{0.5em}{0em} %% Apenas para espaçamento
  Quasi-Poisson & np & QDev & AIC & F & diff np & P($>F$) & $\hat{\sigma}$ \\
  \midrule
<<anova-ovos4, results="asis">>=

auxQP <- lapply(fmodels.ovos, function(x) x$QP)
tab <- do.call("getAnova", c(print = FALSE, auxQP))
tab <- cbind(tab, sapply(auxQP, function(x) summary(x)$dispersion))
rownames(tab) <- paste("Preditor", rownames(tab))
print(xtable(tab, digits = digits),
      include.colnames = FALSE,
      hline.after = NULL,
      only.contents = TRUE)

@
 \bottomrule
\end{tabularx}
\vspace{-1mm}

\footnotesize \raggedright np, number of parameters; diff $\ell$,
difference in log-likelihoods; QDev, quasi-deviance, F, F statistics
based on quasi-deviances; diff np, difference in np.
\end{table}

\autoref{tab:anova-ovos} summarizes the results of fitted models and
likelihood ratio tests comparing the sequence of predictors. All models
indicate the cubic effect of nitrofen concentration. Considering this
predictor, there is an evidence of equidispersed counts, $\phi$
estimates of COM-Poisson close to zero and $\sigma$ of quasi-Poisson, to
one. It is interesting to note that if we omit the effects of a greater
order on the predictor, the modes show evidence of overdispersion
counts. This exemplifies the discussion about causes of overdispersion
made in \autoref{introduction}. We can also note that the quasi-Poisson
approach, although robust to equisdispersion assumption, shows a higher
descriptive levels ($p$-values) than parametric models, that is, the
tests under parametric models are most powerful than quasi-Poisson
models in the equidispersed case.

In \autoref{tab:coef-ovos} we presented the estimates of the regression
parameters considering the cubic dose effect (third predictor). The
interpretations are similar to the discussed in the others cases
studies, but in this the Poisson model is also suitable for indicating
the significance of the effects. In addition, note that the parameters
of the standard COM-Poisson model are comparable to the others models,
this occurs because it is the particular case $\phi = 0$ which implies
$\lambda = \mu$. \autoref{fig:pred-ovos} shows the number of live
off-springs observed in experiment with fitted curves and confidence
bands for all model strategies adopted.

\begin{table}[H]
\centering
\caption{Parameter estimates (Est) and ratio between estimate and
  standard error (SE) for the four model strategies for the nitrofen
  experiment.}
\label{tab:coef-ovos}
\begin{tabularx}{\textwidth}{lCCCCCCCC}
  \toprule
  & \multicolumn{2}{c}{Poisson} &
    \multicolumn{2}{c}{COM-Poisson} &
    \multicolumn{2}{c}{COM-Poisson$_\mu$} &
    \multicolumn{2}{c}{Quasi-Poisson} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
<<coef-ovos, results="asis">>=

#-----------------------------------------------------------------------
# Goodness of fit measures and estimate parameters

# GoF measures
measures.ovos <- lapply(models.ovos, function(x)
    c("LogLik" = logLik(x), "AIC" = AIC(x), "BIC" = BIC(x)))

# Get the estimates
co2 <- coef(m2C2)
est <- lapply(models.ovos, FUN = function(x) getCoefs(x))
est.ovos <- do.call(cbind, est)[-1, ]

# Organize in table
pnames <- paste0("\\beta_{", 0:3, "}")
rownames(est.ovos) <- paste0("$", pnames, "$")
print.xtable(xtable(est.ovos, digits = 4),
             hline.after = c(0, nrow(est.ovos)),
             only.contents = TRUE)

@
\end{tabularx}
\end{table}

The fitted values and confidence bands in \autoref{fig:pred-ovos} come
to have a full overlay. This shows that even in case where is not
necessary to estimate the precision parameter $\phi$, but estimating it
does not lead to incorrect analysis.

<<pred-ovos, fig.height=3.5, fig.width=5.5, out.width="0.7\\textwidth", fig.cap="Number of live offsprings observed for each nitrofen concentration level with fitted curves and 95\\% confidence intervals.">>=

#-----------------------------------------------------------------------
# Prediction

# Data for prediction
pred <- with(nitrofen,
             data.frame("dose" = seq(min(dose), max(dose),
                                     length.out = 100)))
qn <- qnorm(0.975) * c(fit = 0, lwr = -1, upr = 1)

# Design matrix for prediction
X <- model.matrix(update(form3, NULL ~ .), pred)

# Considering Poisson
aux <- exp(confint(
    glht(m3PO, linfct = X), calpha = univariate_calpha())$confint)
colnames(aux) <- c("fit", "lwr", "upr")
aux <- data.frame(modelo = "Poisson", aux)
predPO.novos <- cbind(pred, aux)

# Considering COM-Poisson
aux <- predictcm(m3C1, newdata = X)
aux <- data.frame(modelo = "COM-Poisson", aux)
predC1.novos <- cbind(pred, aux)

# Considering COM-Poisson (mean parametrization)
aux <- predictcm(m3C2, newdata = X)
aux <- data.frame(modelo = "COM-Poisson2", aux)
predC2.novos <- cbind(pred, aux)

# Considering Quasi-Poisson
aux <- exp(confint(
    glht(m3QP, linfct = X), calpha = univariate_calpha())$confint)
colnames(aux) <- c("fit", "lwr", "upr")
aux <- data.frame(modelo = "Quasi-Poisson", aux)
predQP.novos <- cbind(pred, aux)

# Representing the confidence intervals
pred.novos <- rbind(predPO.novos, predC1.novos, predC2.novos, predQP.novos)
ord <- order(pred.novos$dose, pred.novos$modelo)
pred.novos <- pred.novos[ord, ]

# Legend
key <- list(columns = 2,
            lines = list(col = 1, lty = rev(1:4)),
            text = list(parse(
                text = c("'Poisson'", "'COM-Poisson'",
                         "'COM-Poisson'[mu]", "'Quasi-Poisson'"))
                )
            )

# Graph
xyplot(novos ~ dose,
       data = nitrofen,
       xlab = "Nitrofen concentration level",
       ylab = "Number of live offspring",
       grid = TRUE,
       alpha = 0.6,
       key = key,
       spread = 0.05,
       panel = panel.beeswarm) +
    as.layer(
        xyplot(fit ~ dose,
               auto.key = TRUE,
               data = pred.novos,
               groups = modelo,
               type = "l",
               col = 1,
               ly = pred.novos$lwr,
               uy = pred.novos$upr,
               cty = "bands",
               fill = "gray80",
               alpha = 0.1,
               panel = panel.superpose,
               panel.groups = panel.cbH,
               prepanel = prepanel.cbH,
               lty = rev(1:4))
    )

@

Finally, in \autoref{tab:corr-ovos} we presented the empirical
correlations between regression and precision parameters. The results
show that even in the particular case ($\phi=0$), the empirical
correlations for the standard models are not zero. For the
reparametrized model, as discussed in the previous sections, the
correlations are practically null.

<<corr-ovos, results="asis">>=

#-----------------------------------------------------------------------
# Correlation between estimates
corr.ovos <- do.call("rbind",
                     lapply(models.ovos[c("C1", "C2")],
                            function(x) cov2cor(vcov(x))[1, -1]))

# Organize on table
rownames(corr.ovos) <- paste0("COM-Poisson", c("", "$_\\mu$"))
colnames(corr.ovos) <- gsub("(beta)", "hat{\\\\\\1}", pnames)

caption <- paste("Empirical correlations between $\\hat{\\phi}$ and",
                 "$\\hat{\\bm{\\beta}}$ for the two parametrizations",
                 "of COM-Poisson model fit to equidispersed data.")
print(xtable(corr.ovos,
             align = c("lCCCC"),
             caption = caption,
             digits = 3,
             label = "tab:corr-ovos"),
      # size = "small",
      sanitize.rownames.function = identity,
      sanitize.colnames.function = function(x) sprintf("$%s$", x),
      tabular.environment = "tabularx",
      width = "\\textwidth")

@

\section{Concluding remarks}
\label{conclusion}

In this paper, we argued the parametrization of COM-Poisson distribution
to analyse dispersed count data. We proposed a novel reparametrization
based only mean approximation indicated by \cite{Shmueli2005}. The
likelihood-based method to estimation and inference is presented. We
assess the properties of these estimators by a simulation
study. Applications of the proposed model are showed by three case
studies with over, under and equidispersed counts. In the data analysis
we compared standard Poisson, original COM-Poisson, reparametrized
COM-Poisson and quasi-Poisson models.

We computed the quadratic errors for the mean approximation. The results
showed that it is well accurate supports our proposed
reparametrization. The results of a simulation study showed that the
estimators of the regression and precision parameters are consistent and
unbiased and asymptotically unbiased respectively. The empirical
coverage rates of the confidence intervals for the parameters, compute
by the asymptotic distribution of the estimators, are close to the
nominal level for sizes greater than 100. The worst scenario is when we
have small sample size and strong overdispersed counts. In general, we
recommend the use of Wald intervals for computational simplicity and
good results.

The data analysis in the case studies has shown that COM-Poisson is a
suitable distribution for over, under and equidispersion situations. The
observed empirical correlation between regression and precision
parameters suggest the orthogonality between $\mu$ and $\phi$ in
COM-Poisson$_\mu$ distribution against the original parametrization. Due
to this feature the computational procedure is faster under the
reparametrized model. Another advantage, perhaps the main, of the
reparametrization is that the estimated regression parameters have
interpretation of rate ratios, same of the Poisson model. In comparison
of the quasi-Poisson approach the results was similar in both
simulations, nevertheless the COM-Poisson is a full parametric model,
allowing the calculation of probabilities for example.

In general, the results presented by the reparametrized COM-Poisson
models were satisfactory and superior to the conventional
approaches. Therefore, its use in the analysis of count data is
encouraged. The computational routines for fitting of original and
reparametrized COM-Poisson models are available in the supplementary
material\textsuperscript{\ref{papercompanion}}.

There are many possible extensions and possible topics for further
investigation to model discussed in the present paper, including
simulation study to assess model robustness against distribution
miss-specification, asses theoretical approximations for
$Z(\lambda, \nu)$ (or $Z(\mu,\phi))$, in order to avoid the selection of
sum's upper bound, propose a double GLM based on the reparametrized
COM-Poisson model and a accommodate random effects since we have a model
parametrized on mean.

%-----------------------------------------------------------------------
\newpage

\small
\bibliography{references.bib}
\end{document}
